
\documentclass[12pt,a4paper,openright,twoside]{report}
\usepackage{tikz}
\usepackage{amsmath, xparse}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{zref-savepos,graphicx}
\usepackage{lipsum}
\usepackage{listings}


% \filltopageendgraphics[<options>]{<file>}
\newcommand{\filltopageendgraphics}[2][]{%
  \par
  \zsaveposy{top-\thepage}% Mark (baseline of) top of image
  \vfill
  \zsaveposy{bottom-\thepage}% Mark (baseline of) bottom of image
  \smash{\includegraphics[height=\dimexpr\zposy{top-\thepage}sp-\zposy{bottom-\thepage}sp\relax,#1]{#2}}%
  \par
}

\usepackage{afterpage}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per scrivere in italiano
 \usepackage[british, italian]{babel}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per accettare i caratteri
                                        %   digitati da tastiera come è à
                                        %   si può usare anche
                                        %   \usepackage[T1]{fontenc}
                                        %   però con questa libreria
                                        %   il tempo di compilazione
                                        %   aumenta
% \usepackage[latin1]{inputenc}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per impostare il documento
\usepackage{fancyhdr}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per avere l'indentazione
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   all'inizio dei capitoli, ...
\usepackage{indentfirst}
%
%%%%%%%%%libreria per mostrare le etichette
%\usepackage{showkeys}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per inserire grafici
\usepackage{graphicx}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per utilizzare font
                                        %   particolari ad esempio
                                        %   \textsc{}
\usepackage{newlfont}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%librerie matematiche
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amsthm}
%
\oddsidemargin=30pt \evensidemargin=20pt%impostano i margini
\hyphenation{sil-la-ba-zio-ne pa-ren-te-si}%serve per la sillabazione: tra parentesi 
					   %vanno inserite come nell'esempio le parole 
%					   %che latex non riesce a tagliare nel modo giusto andando a capo.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%comandi per l'impostazione
                                        %   della pagina, vedi il manuale
                                        %   della libreria fancyhdr
                                        %   per ulteriori delucidazioni
\pagestyle{fancy}\addtolength{\headwidth}{20pt}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection \ #1}{}}
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\cfoot{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linespread{1.3}                        %comando per impostare l'interlinea
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%definisce nuovi comandi
%
\begin{document}
\begin{titlepage}  
\begin{center}
{{\Large{\textsc{Alma Mater Studiorum $\cdot$ Universit\`a di
Bologna}}}} 
\rule[0.1cm]{14cm}{0.1mm}
%\rule[0.5cm]{15.8cm}{0.6mm}
{\small{\bf SCUOLA DI INGEGNERIA\\
Corso di Laurea in Ingegneria Informatica }}
\end{center}
\vspace{15mm}
\begin{center}
{\LARGE{\bf Modelli Predittivi basati   }}\\
\vspace{3mm}
{\LARGE{\bf  su Computer Vision per Proprietà  }}\\
\vspace{3mm}
{\LARGE{\bf Fisiche di Nanografene }}\\
\end{center}
\vspace{30mm}
\par
\noindent
\begin{minipage}[t]{0.47\textwidth}
{\large{\bf Relatore:\\
Chiar.mo Prof.\\
Paolo Bellavista}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth}\raggedleft
{\large{\bf Presentata da:\\
Gabriele Ceccolini}}
\end{minipage}
\vspace{20mm}
\begin{center}
{\large{\bf Sessione\\%inserire il numero della sessione in cui ci si laurea
Anno Accademico 2022/2023 }}%inserire l'anno accademico a cui si è iscritti
\end{center}
%crea un ambiente libero da vincoli
                                        %   di margini e grandezza caratteri:
                                        %   si pu\`o modificare quello che si
                                        %   vuole, tanto fuori da questo
                                        %   ambiente tutto viene ristabilito
%
\thispagestyle{empty}                   %elimina il numero della pagina
%\topmargin=6.5cm                        %imposta il margina superiore a 6.5cm
\raggedleft                             %incolonna la scrittura a destra
\large                                  %aumenta la grandezza del carattere
                                        %   a 14pt

\afterpage{\blankpage}
\newpage
\emph{Per mio padre Domenico e per mia madre Sara che, fin dall'inizio, hanno sempre supportato e incoraggiato tutte le mie passioni, desideri e scelte.}


\\[3in]

\em

Part of the inhumanity of the computer is that, once it is competently programmed and working smoothly,

it is completely honest. \\

Isaac Asimov
\newpage                                %va in una pagina nuova
\afterpage{\blankpage}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage{\pagestyle{empty}\cleardoublepage}%non numera l'ultima pagina sinistra
\end{titlepage}
% \pagenumbering{roman}    %serve per mettere i numeri romani

\tableofcontents
%\listoffigures                          %crea l'elenco delle figure
\begin{otherlanguage}{british}
\begin{abstract}
  \addcontentsline{toc}{chapter}{Abstract}

    Il tema su cui verte questa tesi sono le attività svolte durante il tirocinio curricolare presso \emph{CNR-ISMN (Istitituto per lo Studio dei Materiali Nanostrutturati)}. \\
    L'obbiettivo del progetto svolto è stato quello di mettere in correllazione le propietà chimico/ficiche di campioni di grafene, provenienti da simulazioni al calcolatore, con
    i \emph{difetti} e \emph{imperfezioni} presenti nella loro struttura atomica.\\
    I sudetti \emph{difetti} infatti si presentano come dei \emph{vuoti} all'interno della struttura atomica, queste \emph{cavità} quindi sono state individuate e analizzate automaticamente utilizzando vari strumenti, fra cui la rete neurale di Object Detection \emph{YOLOv8} e la libreria di Computer Vision \emph{OpenCV}.
    Grazie a questi metodi è stato possibile andare ad analizzare automaticamente i difetti di ogni campione e cercare quindi eventuali correlazioni con le propietà fisiche del campione stesso. 
    Si è andati quindi a sviluppare una rete predittiva in grando di predire propietà chimico-fisiche a partire dalla conformazione strutturale e topologica dei campioni.
\end{abstract}
\end{otherlanguage}

\chapter*{Introduzione}                 %crea l'introduzione (un capitolo
                                        %   non numerato)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries
% INTRODUZIONE}]{\fancyplain{}{\bfseries\thepage}}
% \lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
% INTRODUZIONE}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%aggiunge la voce Introduzione
                                        %   nell'indice
% \addcontentsline{toc}{chapter}{Introduzione}
\addcontentsline{toc}{chapter}{Introduzione}
L'Intelligenza Artificile e in particolare i modelli di Machine Learning basati sulle reti neurali nei tempi recenti si sono dimostrate tecnologie pervasive in quasi tutti gli ambiti di ricerca, dimostrandosi estremamente flessibili e applicabili ai domini più eternogenei e disparati. \\

Non fa eccezione la ricerca sui materiali innovativi come quella sul grafene, un materiale composto da un singolo strato di atomi di carbonio posizionati in conformazione esagonale dando forma ai caratteristici reticoli.
Le propietà e caratteristiche eccezionali di questo materiale derivano da molte delle sue unicità, quali per esempio il suo sottilissimo spessore lungo solo un atomo e di conseguenza il suo peso estremamente contenuto. \\
Queste propietà \emph{bidimensionali} conferiscono al materiale resistenza, flessibilità e leggerezza senza concorrenza tra gli altri materiali tradizionali. \\
Le possibili applicazioni di un simile materiale sono anchesse estramamente multi-dominio e spaziano dai supercondensatori, semiconduttori, nonchè per usi strutturali e tanto altro. 
Per la natura stessa del materiale, la produzione su scala industriale risulta tutt'altro che banale e i costi elevati. \\
Spesso durante la produzione non si riesce ad ottenere un campione di grafene totalmente privo di impurità. E' inoltre comune trovare danni strutturali sul reticolo atomico con conseguente presenza di cavità e imperfezioni sullo stesso. \\

L'idea del progetto quindi si è basata sull'analisi di campioni di grafene difettato derivanti da simulazioni al calcolatore. Grazie alla conoscenza a priori delle propietà fisiche di ogni campione, originariamente calcolate dalle simulazioni stesse è stato possibile mettere in correlazione la topologia e i tratti geometrici dei suddetti difetti con le propietà del materiale stesso.\\
In primo luogo, prima di andare ad analizzare le geometrie dei vari difetti è risultato utile individuarli automaticamente. 
Questo è stato fatto addestrando la rete neurale YOLOv8, potente modello di Object Detection noto per la sua velocità di inferenza e per le elevate performace ottenibili dopo un training relativaemnte breve e con campioni limitati.\\
Una volta individuate ed estratte dallo sfondo le regioni di interesse si è proceduto con l'analisi geometrica e topologica dei difetti con la libreria di Computer Vision OpenCV.
Questa libreria multipiattaforma inizialmente sviluppata da Intel e divenuta successivamente open-source offre numerose funzioni e metodi per la manipolazione e analisi di immagini, nel caso di questo progetto ci si è interfacciati ad essa tramite Python. \\
E' stato quindi possibile estrapolare tratti geometrici e topologici dai difetti in questione calcolandone dai tratti più semplici come area e perimetro fino a quelli più sofisticati quali eccenticità, circolarità e molto altro.\\
I dati sono stati integrati in un dataframe \emph{Pandas}, grazie ad esso sono state fatte analisi sulle correlazioni fra i parametri stessi. Trovare parametri molto correlati fra loro è importante per evitare di selezionare valori ridondanti nell'addestramento del modello predittivo.\\
Dopo avere scelto le features più significative si è andati quindi ad addestrare un regressore ad alberi decisionali con lo scopo di predire il valore numerico dell'energia totale di un campione a partire dalle propietà geometriche selezionate. Il predittore si è dimostrato adatto al problema richiesto, con una precisione delle predizioni soddifacente.
Infine si è andati ad esplorare la possibilità di generalizzare l'intero progetto all'analisi di immagini reali derivanti dalla microscopia, impostando eventuali estesioni future con conseguenti possibili ricadute industriali dello strumento. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
\clearpage{\pagestyle{empty}\cleardoublepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
INDICE}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \listoftables                           %crea l'elenco delle tabelle
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}


%------------------------------------------------------------------------------------------------------------- CAPITOLO 1 -------------------------------------------------------------------------------------------------
\chapter{Introduzione al Machine Learning}                %crea il capitolo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries\rightmark}}
\pagenumbering{arabic}                  %mette i numeri arabi
La disciplina di ricerca sull'Intelligenza artificale pone le sue radici già a metà del XX secolo. L'obbiettivo generale di allora era quello di sviluppare macchine capaci di eseguire mansioni tipicamente attribute e svolte da umani. 
Con il passare dei decenni l'obbiettivo è rimasto lo stesso ma si sono esplorate numerose vie e tecniche differenti anche molto diverse tra loro, fra cui la sottocatgoria denominata come \emph{Machine Learning}. Quest'ultima è l'insieme delle tecniche e dei metodi statistici con i quali si va migliorare un algoritmo che ha come obbiettivo quello di identificare pattern nei dati. 
Questi metodi si sono riveati particolarmente utili in quei campi dove è notoriamente difficile se non impossibile sviluppare soluzioni tramite algoritmi tradizionali, per esempio il riconoscimento vocale e di immagini.\\
Negli ultimi decenni con il rapido avanzamento della velocità di calcolo a disposizione queste tecnologie hanno ricevuto una notevole accellerazione nello sviluppo e nei risultati raggiunti. 
In particolare ha preso sempre più popolarità l'approcio denomicanto come Deep Learning, ovvero quei metodi basati su reti neurali artificiali su più \emph{strati}. Ognuno di questi strati estrapola gerarchie di inforamzioni di un certo livello definite a loro volta su quelle di livello inferiore calcolate dagli strati precedenti.
\section{Reti neurali}
Una rete neurale consiste in un insieme di \emph{neuroni} (detti anche \emph{nodi}) collegati fra loro, che concorrono per risolvere un problema.
Ogni nodo può essere pensato come una semplice funzione di tante variabili di ingresso quanti sono i nodi collegati ad esso. \\
Il suo valore di attivazione quindi sarà la somma pesata dei valori di attivazione dei nodi che prende come input, più la somma di un valore chiamato \emph{bias}, il tutto all’interno di una \emph{funzione di attivazione} non lineare. \\
In formule l’attivazione di un neurone può essere espressa dalla semplice formula: \\
\begin{equation}
    y = \sigma(w \cdot x + b)
\end{equation}
Dove, \\
\begin{itemize}
    \item 
        $y$ è il valore scalare di attivazione del nodo, esso può variare con continuità da 0 a 1, dove 0 rappresenta la totale assenza di attivazione del nodo, mentre 1 rappresenta l'attivazione massima possibile. 
    \item 
        $ x= \begin{bmatrix} x_1 , x_2 , \cdot\cdot , x_n\end{bmatrix} $ rappresenta il vettore di input, ovvero i valori di attivazione $x_1, x_2, ... , x_n$ degli $n$ neuroni connessi al neurone in questione.
    \item 
        $ w= \begin{bmatrix} w_1 , w_2 , \cdot\cdot , w_n\end{bmatrix} $ della stessa dimensione di $x$
        reppresenta il vettore dei $pesi$, questi parametri sono importanti in quanto venendo moltiplicati per i rispettivi valori di input permettono di calcolare la media pesata dei valori di ingresso.
    \item 
        $b$ è detto $bias$, un valore che si va a sommare semplicemente alla somma pesata degli input.
    \item 
        $\sigma$ è la \emph{funzione di attivazione}, ne esistono di numerosi tipi, il suo compito principale è quello di aggiungere una componente di \emph{non-linearità} alla rete stessa. 

\end{itemize}
 Graficamente l'attivazione di un nuerone con soli tre input può essere rappresentata come: \\
 \begin{center}
     \begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2] 
  (x2) {$x_2$};
\node[on chain=2,join=by o-latex] 
  {$w_2$};
\node[on chain=2,init] (sigma) 
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Funzione di \\ attivazione}}]   
  {$\sigma$};
\node[on chain=2,label=above:Output,join=by -latex] 
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm) 
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex] 
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm) 
  (x3) {$x_3$};
\node[on chain=3,label=below:Pesi,join=by o-latex] 
  (w3) {$w_3$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
\end{tikzpicture}
 \end{center}

Sempre in formule si può rappresentare l'attivazione del neurone in forma estesa come: 
\begin{equation}
    y = \sigma(w_1  x_1 + w_1  x_1 + .. w_n  x_n + b) = \sigma(\sum_{n=1}^{n} w_n x_n + b)
\end{equation}
La funzione di attivazione è necessaria innanzitutto per trasformare un valore \emph{unbunded}, ovvero senza un valore massimo, derivante dalla sommatoria degli input in un valore continuo che va da 0 a 1, ovvero il valore di attivazione finale che vogliamo fare assumere al neurone del caso. \\
Esistono numerose funzioni di attivazione, una delle più famose e semplici è la \emph{Sigmoide} \[ S(x) = \frac{1}{1+e^{-x}}\] simile alla funzione \emph{gradino unitario} ma in versione smussata, senza la discontinuità nello zero. Questa funzione rispetto al \emph{gradino unitario} ha vari vantaggi, in particolare a piccole variazioni dell'input avremo piccole variazioni nell'output. \\
Nelle reti più moderne è sempre più utilizzata la RELU, ovvero la funzione rettificatore espressa semplicemente come: 
\begin{equation}
     f(x)=\max(0,x)
\end{equation}
Quest'ultima si è dimostrata come la più efficace nelle reti di apprendimento profondo. Fra i suoi vantaggi abbiamo l'attivaazione scarsa, infatti essa si attiva solo nella metà dei campioni di una rete inizializzata casualmente inoltre si hanno meno problemi in fase di discesa del gradiente nella fase di adestramento rispetto alla funzione Sigmoide dei quali parleremo più avanti nel capitolo.
\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{gradino.png}

(a) Gradino unitario
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{sigmoid.png}

(b)  Sigmoide
\end{minipage}
\caption{Comparazione fra funzione gradino unitario e sigmoide }
\end{figure}


Esistono poi funzioni di attivazione più sofisticate, come la Softmax.
Quest'ultima è spesso utile nelle reti classificatrici multiclasse. In questi casi i nodi del layer di output rappesentano le probabilità di classificazione per ciascuna classe. In questi casi è cruciale che la sommatoria dei valori di attivazione dei nodi di output sia uguale a 1.
\newpage
Nel caso generale abbiamo almeno un layer di nodi di input, uno di output e acluni layer intermedi detti \emph{hidden-layer}.
In una rete \emph{fully-connected} tutti i nodi nell n-esimo layer sono connessi a tutti i nodi dell'n-esimo layer. \\
Le feature dei vari pattern estrapolati dalla rete vengono calcolati dai nodi dei layer interni i quali estrapolano propietà sempre più astratte e di alto livello quanto più è avanzata la posizione del layer in cui risiedono. \\
Il vettore dei valori di attivazione di un \emph{hidden-layer} può essere espresso come il prodotto tra la matrice dei pesi del layer in questione e il vettore di attivazione del layer precedente più il vettore dei bias, tutto dentro la funzione di attivazione scelta per questo layer.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{neural_networks-004.png}
\caption{Rete neurale \emph{fully-connected} con $3$ \emph{hidden layers} da $m$ nodi ciascuno.}
\end{figure}

\newpage
In formule il vettore dei valori di attivazione del primo \emph{hidden-layer} può essere espresso come:
    \begin{equation} 
     \begin{pmatrix}
        a_{1}^{(1)} \\[0.3em]
        a_{2}^{(1)} \\
        \vdots \\
        a_{m}^{(1)}
      \end{pmatrix}
    =
      \sigma \left[ 
      \begin{pmatrix}
        w_{1,0} & w_{1,1} & \ldots & w_{1,n} \\
        w_{2,0} & w_{2,1} & \ldots & w_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        w_{m,0} & w_{m,1} & \ldots & w_{m,n}
      \end{pmatrix}
      {
      \begin{pmatrix}
        a_{1}^{(0)} \\[0.3em]
        a_{2}^{(0)} \\
        \vdots \\
        a_{n}^{(0)}
      \end{pmatrix}}
      +
      \begin{pmatrix}
        b_{1}^{(0)} \\[0.3em]
        b_{2}^{(0)} \\
        \vdots \\
        b_{m}^{(0)}
      \end{pmatrix}\right] 
      \\
    \end{equation}
In notazione compatta abbiamo: 
    \begin{equation}
        \mathbf{a}^{(1)} = \mathbf{W}^{(0)} {a^{(0)}}+\mathbf{b}^{(0)}
    \end{equation}

In generale non tutti le reti sono \emph{fully-connected}, spesso si opta per una soluzione ibrida dove si utilizzano strati con architetture diverse per risolvere un problema. 
Un esempio di un'altra archiettura di rete sono le reti ricorsive (RNN), queste hanno dei collegamenti in retroazione capaci di mantenere uno \emph{stato} e quindi avere \emph{memoria}. Mantenere una memoria è cruciale quando gli imput passati in precedenza nella rete in relazione con quelli attuali hanno un significato semantico importante, come per esempio nell'analisi di un video o per un riconoscimento vocale partendo da un file audio.
Un'altro tipo di rete molto importante e centrale in questo lavoro di tesi sono le reti convoluzionali CNN. Esse si basano su filtri convoluzionali che collegano gli \emph{hidden-layer}.\\ Queste reti riescono a estrapolare pattern grafici a partire da un'immagine fornita come imput, per esempio utilizzando il valore numerico associato ad ogni pixel in un'immagine in scala di grigi.\\
Questo tipo di reti sono state usate nell'ambito di questo progetto per identificare automaticamente le aree di interesse dove sono presenti i difetti del grafene. Di queste reti e in particolare di YOLOv8 ne parleremo nel dettaglio nel prossimo capitolo.

\newpage
\section{Algoritmi di apprendimento}
\subsection{Funzione di costo}
Uno dei parametri più importanti nonchè alla base degli algoritmi di apprendimento è la cosidetta \emph{funzione di costo}.
Ne esistono di vari tipi ma intuitivamente essa è una funzione reale che prende come variabili i parametri del sistema $C(\omega,b)$ e fornisce un'indicazione su quanto bene la nostra rete stia performando sui campioni che gli vengono dati.\\
Questa valutazione è possibile confrontando i valori $y(x)$ che la rete produce per gli imput $x$, conoscendo il vettore $a(x)$ dei risultati attesi per gli imput in questione.
Una delle più utilizzate è lo \emph{scarto quadratico medio}, essa fa la media aritmetica del quadrato dell'errore di output rispetto a ogni imput.
\begin{equation}
    C(\omega,b) = \frac{1}{n} \sum_{n=1}^{n} (y(x_{i}) - a_{i}))^2
\end{equation}
Al variare dei numerosi parametri $(W,b)$ la performance della rete varia e così anche la funzione di costo. In particolare più il valore si avvicina a zero e più la rete sta performando bene. \\
Risulata quindi chiaro che minimizzare la funzione di costo $C(\omega,b)$ è il fulcro del problema dell'addestramento delle reti neurali.
\subsection{Discesa del gradiente}
Siamo quindi di fronte a un problema di ottimizzazione di una funzione reale in tante variabili quanti sono i parametri della rete, dove si cerca quindi un minimo della funzione stessa.
Intuitivamente l'inverso del gradiente ci da un'indicazione \emph{spaziale} su dove dobbiamo \emph{muoverci} per avvicinarci il più velocemente possibile al più vicino minimo locale, quindi per quale vettore di valori dobbiamo sommare i nostri parametri $(\omega,b)$ per migliorarli. \\
\newpage
Le iterazioni del metodo si presentano nella forma seguente: prima si va a calcolare l'inverso del gradiente, lo si moltiplica per una certa costante $\eta$, quindi lo si somma ai parametri $(\omega,b)$. In questo modo si vanno ad ottenere i nuovi parametri $(\omega', b')$. \\
L'equzione seguente mostra un'iterazione del processo di discesa del gradiente, dove vengono calcolati i nuovi parametri $(\omega', b')$:
\begin{equation}
    (\omega', b') := (w,b)-\eta \nabla C(w,b)
\end{equation}
Importante notare come l'entità dello \emph{spostamento} per ogni step è arbitrariamente piccola rispetto al valore di $\eta$. Quest'ultimo prende il nome di \emph{learning-rate} ed è un \emph{iperparametro} in quanto è una variabile scelta a priori da colui che sta addestrando la rete. \\
Un'altro accorgimento per evitare fluttuazioni imprevedibili dei parametri è quello di non applicare i cambiamenti ad ogni predizione ma fare la media su un batch di più predizioni. \\
Il metodo illustrato sopra prende il nome di \emph{Gradient Descent GD}\cite{K1}, questo è un metodo applicabile quando la funzione di costo è sufficientemente semplice e il suo gradiente è di facile computazione.\\
Nel \emph{GD} si calcola il gradiente per l'intero set di parametri ed essi vengono aggiornati ad ogni iterazione oppure facendo la media su batch di un certo numero di predizioni. \\
Quando invece ho dataset molto grandi oppure non ho un'espressione della funzione di costo abbastanza semplice si può optare per il metodo noto come \emph{Stochastic Gradient Descent SGD}.
Questo metodo approssima il calcolo del gradiente considerando ogni volta un batch casuale di paramertri ad ogni iterazione, solo una parte dei parametri quindi vengono aggiornati durante un'iterazione.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{Stogra.png}
\caption{Esempio di andamento della funzione costo nel corso delle iterazioni in un problema di ottimizzazione risolto tramite SGD}
\end{figure}
\newpage
Come si può vedere dall'andamento della funzione di costo durante le iterazioni del SGD, non è assicurato il miglioramento della rete ad ogni iterazione, in alcuni casi infatti la funzione di costo può momentaneamente salire, questo è dovuto alla natura stocastica, quindi probabilistica del metodo.\\
In generale comunque, per computare la derivata parziale di un parametro occorre un passaggio e il tutto deve essere ripetuto per ogni epoca. \\
Avendo spesso miliardi di parametri e centinaia di epoche i metodi di GD possono risultare facilemente costosi computazionalemente. \\
Per mitigare questo è stato creato un nuovo algoritmo utilizzabile insieme al SGD, esso prende il nome di \emph{Backpropagation}\cite{backprop}.
Questo algoritmo ricorsivo sfruttando la regola della \emph{chain rule} (durante il calcolo del gradiente) permette di calcolare tutte le derivate e quindi l'intero gradiente in un singolo passaggio.\\

Qui sotto mostro la differenza intuitiva fra la progressione di un algoritmo di GD a batch con uno di SGD.\\
Notare bene come la visualizzazione è possibile solo se si considera il caso estremamente semplificato di soli due parametri. \\Nel caso generico con più di due parametri (anche miliardi) sarebbe impossibile proporre una visualizzazione di uno spazio con così tante dimensioni.
\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{batchGD.png}

(a) Batch Gradient Descent
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{SGD.png}

(b)  Stochastic Gradient Descent
\end{minipage}
\caption{Comparazione fra iterazioni di GD a batch e SGD }
\end{figure}
Notare la natura intrisecamente probabilistica degli step del SGD, essi non assicurano un miglioramento della loss ad ogni step ma il risulato finale può risultare comunque più efficente della controparte GD se analizzato nel complesso.
\newpage

\section{Esempi applicativi di Reti Neurali}
Le reti neurali hanno numerose ricadute per la risoluzione di problemi applicativi di ogni genere.
Forse il più famoso caso applicativo in letteratura e sicuramente il più usato per introdurre le Reti Neurali Convoluzionali (\emph{CNN}) è il problema del riconoscimento automatico di numeri (cifre) scritte a mano. 
Le CNN verranno approfondite nel prossimo capitolo, in particolare verranno inquadrate nella classe di problemi di Object Detection, ma vale comunque la pena introdurre fin da subito la risoluzione di questo ben noto caso di studio. 
\subsection{Classificazione di cifre scritte a mano}

Il dataset di riferimento è il ben noto \emph{MNIST}.\\
Le immagini contenute rappresentano semplici numeri a una cifra scritti a mano, ognuno di questi viene fornito con la relativa annotazone (\emph{label}) scritta da operatori umani.\\
Questo database contiene 60.000 immagini 28x28 pixel in scala di grigi per la fase di training e ulteriori 10.000 per la fase di valutazione del modello. \\
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{minst.png}
\caption{Database MINST}
\end{figure}
La valutazione di ogni modello di reti neurali non deve essere effettuata sugli stessi dati di addestramento, ma su altri che il modello non ha mai visto prima. Ciò che si vuole valutare infatti è la capacità acquisita dal sistema nel riconoscimento di pattern generali estrapolabili da campioni generici. Se utilizzassimo gli stessi dati usati nell'adestramento per valutare la rete, rischieremmo di incorrere nella possibilità che la rete sia performate nell'analisi dei dati di training ma incapace di generalizzare su dati generici mai visti prima.   \\
\newpage
L'imput layer è composto da tanti nodi quanti sono i pixel dell'immagine, ognuno di questi prende il valore numerico in scala di grigi del pixel in questione.
L'imput layer quindi, si deve pensare come la matrice di pixel dell'immagine iniziale. Nel caso avessimo un'immagine a colori, per esempio in formato RGB, al posto di un imput layer sotto forme di matrice, ne avremo 3, una per ogni colore (rosso, verde, blu), lavoreremo quindi con tensori.\\
\newpage
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{pixel-values.png}
\caption{Imput layer di una CNN per analisi immagine}
\end{figure}
Questo problema, come in tutti i casi in cui si cercano oggetti e/o pattern all'interno di immagini, si risolve impiegando strati convoluzionali. \\
Questi strati di neuroni sono composti da \emph{filtri} che \emph{scorrendo} sui valori di attivazione dei layer prededenti computano operazioni di convoluzione estrapolando tratti \emph{grafici} dalle immagini.\\
La natura di questi \emph{filtri} e delle operazioni di convoluzione effettuate da essi verranno approfonditi e esplorati nel prossimo capitolo. 
In generale ci aspettiamo che layer convoluzionali di strati più avanzati estrapolino tratti grafici più astratti e complessi sulla base dei tratti di più basso livello estratti dai livelli inferiori. Nel nostro caso è plausibile che i primi livelli riconoscano feature di basso livello come i bordi delle cifre, mentre quelli successivi riconoscano pattern più strutturati come per esempio i \emph{cerchi} dello zero e dell'otto. 
\newpage

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{digit_arch.png}
\caption{Modello di CNN per riconoscimento di cifre scritte a mano}
\end{figure}
Come mostrato in figura, spesso fra un layer convoluzionale e il successivo sono presenti dei layer di altra natura, come quelli di \emph{pooling}. Ne esistono di vari tipi ma anchessi si comportano come dei filtri che scorrono sulle immagini prodotte dai filtri convoluzionali precedenti. Il loro scopo è quello di diminuire le dimensione delle immagini prodotte mantenedo parte dell'informazone grafica, magari enfatizzando delle strutture geometriche piuttosto che altre.

Gli ultimi layer invece tornano ad essere \emph{fully-connected} e vengono chiamati \emph{DL, Dense Layers.}
In particolare l'ultimo layer si occupa della classificazione della cifra data come input. Come spiegato in precedenza in questi casi si utilizzano funzioni di attivazione con un comportamento similare alla Softmax. La sommatoria dell'attivaizone dei 10 nodi dell'output layer deve essere infatti pari a 1, dato che ciascunio dei valori di attivazione rappresenta la probabilità che l'immagine di imput corrisponda alla cifra simboleggaita dal suddetto nodo.\\
\newpage
In conclusione possiamo dire che l'applicazione di una CNN a questo tipo di problema si rivela la scelta migliore, in generale risulta la scelta più performante in tutti i tipi di probelmi dove l'imput è composto da un'immagine.\\
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{cnn_perf.png}
\caption{Modello di CNN per riconoscimento di cifre scritte a mano}
\end{figure}
In questo caso di studio già dopo una poche decine di epoche la precisione della classificazione si avvicina al 100\%.
Proseguire oltre non è necessario e anzi può risultare controproducente in quanto si potrebbe andare incontro a problemi di overfitting.
Con overfitting si intende ls situazione in cui il modello si adatta troppo ai dati di training perdendo generalità e capacità di astrarre su dataset mai visti prima. \\

%------------------------------------------------------------------------------------------------------------- CAPITOLO 2 -------------------------------------------------------------------------------------------------
{\let\clearpage\relax \chapter{Object Detection, YOLO e applicazioni su campioni di nano grafene}}

In questo capitolo andremo a introdurre la classe di problemi di visione artificiale denominata come Object Detection. Ovvero le soluzioni e i metodi che puntano a individuare, classificare o comunque riconoscere oggetti e/o pattern all'interno di un'immagine o un video. \\
Questa clase di problemi, come accennato nel capitolo precedete si è rivelata molto adatta all'impiego delle CNN (Convolutional Neural Networks), che grazie alla loro capacità di riconiscemnto ed estrapolazione di fetaures grafiche dalle immagini riescono a ottenere ottime performance su questa classe di problemi.\\
Dopo un'introduzione all'architttura delle CNN, ci focalizzeremo sullo studio di \emph{YOLO (You Only Look Once)}, una CNN allo stato dell'arte per quanto riguarda l'Object Detection, particolarmente nota per il basso tempo di inferenza e per questo particolarmente utile nelle applicazioni in tempo reale.\\
Analizzeremo inoltre l'applicazione di questo modello al caso di studio del progetto su cui si basa questa tesi, ovvero sul riconoscimento automatico di difetti su campioni di nano grafene, riportando metodologia e risultati.
\section{Reti neurali convoluzionali}
La struttura delle CNN è composta da vari tipi di layer di natura diversa, come i layer convoluzionali, i pooling layer e i fully-connected layers. 
Ogni tipologia di layer svolge ruoli e opera in modo diverso rispetto agli altri al fine di raggiungere l'obbiettivo preposto.
\\L'architettura generale può essere espressa come come nella figura sottostante.\\

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{CNN_main.jpeg}
\caption{Archiettura di una CNN a singolo canale}
\end{figure}
In questa trattazione delle CNN ci limiteramo ad analizzare il caso in cui l'immagine di input sia in scala di grigi e quindi il layer di input sarà composto da matrice di interi con valori compresi tra 0 e 255.
Nonostante questa semplificazione è importante sapere che il tutto può essere esteso ad immagini a più canali introducendo ulteriori matrici, una per ogni canale aggiuntivo. Generalmente quindi si avranno tensori con 3 dimensioni di profondità. 
\newpage
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{greyscaletorgb.png}
\caption{Passaggio da matrice di imput in scala di grigi a tensore per immagine RGB}
\end{figure}

In questa sezione andremo ad analizzare un tipo di layer alla volta, iniziando dai layer convoluzionali.
\subsection{Layer convoluzionali}
I componenti fondamentali dei layer convolzuonali non sono più vettori di neuroni ma fetaure map (matrici o tensori) computate da dei \emph{filtri} tramite operazioni di convoluzione discreta.
Questi filtri si presentano come delle matrici di dimensione minore della matrice di input, il loro comportamento è quello di \emph{scorrere} su tutta l'immagine di partenza andando a creare un'altra matrice, di dimensione minore chiamata \emph{feature map.}
L'operazione di convololuzione si può esprimere com la sommatoria del prodotto puntuale di ogni valore del filtro con i rispettivi valori nella matrice di partenza, secondo la posizione del filtro.
\newpage
In formule possiamo indicare l'attivazione di ogni nodo (pixel) della feature map come: 
\begin{equation}
    o_{i,j} = \sigma(b + \sum _{l=0}^{n}\sum _{m=0}^{n}\omega_{l,m}a_{i+l, j+m})
\end{equation}
dove $o_{i,j}$ è l'attivazione dell'elemento della matrice di output, $\sigma$ è una qualsiasi funzione di attivazione, $b$ l'eventuale bias, $\omega_{l,m}$ il valore dell'elemento del filtro nella posizione $l,m$ e $a_{i+l, j+m}$ il valore del pixel dell'immagine di input che verrà moltiplicato per $\omega_{l,m}$ durante lo step.

\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{filtera.png}

(a) Prima convoluzione  
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{filterb.png}

(b) Seconda convoluzione
\end{minipage}
\caption{Prime due iterazioni di convoluzione per la creazione di una feature map, con un filtro 3x3, no padding e stride 1}
\end{figure}
La feature map prodotta risulta più piccola dell'immagine di partenza, questo può essere un problema se si applicano numerose convoluzioni in sequenza andano quindi a ridurre sempre di più la dimensione delle immagini prodotte. Per evitare questo si può applicare la tecnica chiamata \emph{padding}, ovvero l'aggiunta a  posteriori di uno strato esterno alla matrice dell'immagine con valori nulli (\emph{zero padding}. Allo scorrere del filtro quindi si otterrà un'immagine della stessa dimensione di partenza.\\
Lo \emph{stride} invece è la dimensione dello \emph{step} del filtro durante lo scorrimento, solitamente è pari a 1, se lo si aumentasse si otterebbero anche in questo caso feature map più piccole dell'originale.

Questi filtri nei primi layer sono in grado di estrarre tratti di basso livello come i bordi e contorni degli oggetti. In generale sono molto adatti per enfatizzazione contorni di oggetti sull'immagine di partenza in modo da rendere più facile le operazioni dei layer successivi.
Nell'immagine successiva si mostrano varie feature map derivate da diversi filtri diversi. E' chiaro come l'operazione di convoluzione applicata da specifici filtri sia in grado di mettere in risalto contorni diversi rispetto ad altri.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fetaureMaps.png}
\caption{Fetaure map generate da diversi filtri a sulla stessa immagine}
\end{figure}
Il risulatato dell'operazione lineare di convoluzione viene quindi passato per una funzione di attivazione non lineare come la ReLU, discussa nel capitolo precedente.\\

I parametri fondamentali di apprendimento delle CNN sono propio i pesi (quindi gli elementi di matrice) dei filtri convoluzionali, la rete quindi apprenderà i valori ottimali per ciascun filtro allo scopo di estrapolare i pattern richeisti dal problema.
\\
Invece, la dimensione e il numero dei filtri, il padding e lo stride sono \emph{iperparametri}, ovvero sono valori decisi a priori da colui che sta addestrando la rete, prima del processo di training stesso. 
\newpage
\subsection{Layer di pooling}
Dopo una serie di convoluzioni, tipicamente, si va ad applicare un layer di \emph{pooling}, ovvero un'altra operazione lineare simile alla convoluzione dalla quale si ottengono immagini più piccole ma che comunque mantengono un certo contenuto informativo posizionale della feature map iniziale.
Esistono varie tipologie di pooling con comportamenti diversi tra loro, nell'immagine sottostante si mostra il funzioanamento della tipologia di gran lunga più utilizzata, il \emph{Max-pooling}.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{maxpooling.png}
\caption{Maxpooling 2x2, stride 2}
\end{figure}
Il funzionamento del max-pooling è similare a quello di un filtro convoluzionale che scorre su una feature map, ma al posto di fare la convoluzione dei valori, prende il valore più grande fra quelli coperti dalle dimensioni del filtro e lo riporta nella nuova immagine.\\

Il \emph{pooling}, più in generale, è quindi un processo di scalamento della feature map di partenza introducendo una invarianza a piccole distorsioni e variazioni dell'immagine stessa mantenendo però gran parte delle informazioni spaziali della dell'immagine iniziale.\\
Grazie a questo processo è possibile ridurre notevolemente i parametri di apprendimento della rete. Notare inoltre che i parametri di pooling come dimensione dei filtri, stride e padding sono tutti iperparametri che non vengono appresi dalla rete in fase di training.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{maxPoolingResult.png}
\caption{Risultato di un'operazione di max-pooling 2x2 con stride 2}
\end{figure}
\\Notare come se si va ad applicare un layer di max-pooling 2x2 con stride 2, la feature map di partenza viene dimezzata come dimensioni ma il contenuto semantico dell'immagine rimane intatto.

\subsection{Fully-connected layer }
Gli ultimi layer della rete sono fully-connected, del tutto simili a quelli introdotti nel capitolo 1. \\
Le feature map prodotte dall'ultimo layer convoluzionale/pooling vengono \emph{appiattite}, ovvero inserite in un semplice array unidimensionale (un vettore) e connessi a uno o più layer fully-connected. In questo layer per ogni connessione fra un neurone e l'altro c'è un peso, che la rete andrà ad apprendere durante l'addestramento, inoltre l'attivazione del singolo neurone viene passata tipicamente attraverso la ReLU. Le feature grafiche estratte dai layer precedenti sono finalmente mappate tramite i layer fully-connected al layer di output che rappresenta le probabilità di classificazione dell'immagine.
\\Il layer di output viene anchesso passato per una funzione di attivazione, tipicamente diversa da quelle utilizzate dei dense-layer precedenti. Nel caso della classificazione multipla si usa la Softmax, nel caso della classificazione binaria la sigmoide.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{CNN_fully_connected.png}
\caption{CNN con fully connected layer nella parte finale per classificazione multiclasse di oggetti }
\end{figure}

Nei problemi di classifcazione multiclasse il layer di output può essere trattato come una distribuzione di probabilità. Ovvero la rete non si limita a fornire un giudizio sulla classe presente nell'immagine, ma fornisce una probabilità per ogni classe presente nel problema.
Per valutare la performace della rete è utile quindi confrontare la distribuzione di probabilità prodotta dalla rete con quella attesa.
\newpage
Viene usata quindi una funzione di costo diversa da quella trattata nel primo capitolo, chimata funzione di cross-entropia.
\begin{equation}
    C\left(p\|q\right)=-\sum _{i}^{M}p_{i}\log _{2}q_{i}
\end{equation}
All'interno del contesto di una predizione per un'immagine in input, $M$ rappresenta il numero di classi globali, $p_{i}$ rappresenta il valore della classe $i$ nella distribuzione dei valori attesi, e $q_{i}$ rappresenta il valore della classe $i$ nella distribuzione prodotta dalla rete (predizione).\\
La formuala nel caso pratico dei problemi di clasificazione si semplifica in quanto i valori di $p_{i}$ sono sempre nulli per tutte le classi meno che per la classe effettivamente indicata nella fase di labeling per quell'immagine dove invece è pari a 1. \\ Quindi possiamo riscrivere la formula come: 
\begin{equation}
    C\left(p\|q\right)=-\sum _{i}^{M}\log _{2}q_{i}
\end{equation}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{cross_entropy.png}
\caption{Comportamento della funzione di costo in base alle predizioni del modello}
\end{figure}
La cross-entropy si è rivelata molto adatta nei problemi di classificazione multiclasse per la sua rapida pendenza nei casi in cui la rete assegna una scarsa probabilità alla classe presente nel'immagine. In questi scenari infatti le magnitudine delle sue derivate inducono a rapide variazioni dei parametri di apprendimento. In questo modo gli algoritmi di ottimizzazione riescono a migliorare le performace della rete con rapidità.
\newpage
Concludo la sezione osservando che nelle CNN è fondamentale scegliere a priori un buon set di iperparametri e di architettura dei layer adatto per risolvere il problema in questione, compresa la funzione di costo e l'ottimizzatore. \\

Nella prossima sezione andremo ad analizzare il problema dell'Object Detection e l'architettura di una nota rete di Object Detection, YOLO.
\newpage
\section{Object Detection}
I problemi di Object Detection consistono nel volere individuare da un'immagine data come input uno o più oggetti, disegnando rettangoli che li ricoprono completamente, chiamati \emph{bounding boxes}. Inoltre, nel caso di un problema multiclasse, il modello dovrà classificare correttamente l'oggetto trovato tra le varie classi possibili. Dunque, viene fornito anche un valore numerico compreso tra 0 e 1 che rappresenta la confidenza del modello per la suddetta predizione.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{object_detection.png}
\caption{Predizione di un modello di Object Detection multiclasse}
\end{figure}

Da un'immagine fornita come input a un modello di Object Detection, verrà prodotto un file di testo, dove in ogni riga, sarà riportata una predizione di un'oggetto presente nell'immagine.
Per ciascuno degli oggetti individuati verrà riportato il numero identificativo della classe dell'oggetto (nel caso esista una sola classe il numero sarà sempre 0), la posizione dell'oggetto in formato xywh (posizione spaziale del centro e larghezza e altezza della box) oppure xyxy (coordinate rispettivamente dell'angolo superiore sinistro e inferiore destro della bounding box) e infine il valore di confidenza del modello per l'oggetto in questione. Questo ultimo parametro è particolarmente importante perchè permette a colui che utilizza o addestra la rete di ingorare predizioni al di sotto di un certo coeefficente di confidenza.\\ E' evidente come la scelta del coefficente di confidenza minimo sia molto delicata in quanto un valore alto riduce il numero di falsi positivi ma aumenta i falsi negativi e viceversa.\\
Formalizzando i concetti appena citati, nella prossima sezione definiremo i parametri fondamentali per la valutazione di un modello di Object Detection.
\subsection{Parametri di valutazione di un modello di Object Detection}
Le due metriche fondamentali per analizzare la perfomance di un modello sono la \emph{Precision} e la \emph{Recall}.
Rispettivamente, la \emph{Precision} fornisce un'indicazione sulla effettiva veredicità sulle predizioni fatte dal modello, la \emph{Recall} invece, ci da un'indicazione sul rapporto tra gli oggetti effetivamente individuati dalla rete e tutti quelli presenti nelle immagini.\\
Queste due importanti metriche, mostrano la loro correlazione nella curva \emph{Precision Recall curve}.
Formalizzando in formule abbiamo: 
\begin{align*}
Precision& = \frac{TP}{TP + FP}          &  Recall = \frac{TP}{TP + FN} \\
TP& = True \; positive    &  FN = True \; negative\\
FP& = False \; positive    &  FN = False \; negative
\end{align*}
Da questa curva, integrando è possibile ricavare un parametro molto informativo sulla qualità del modello, l'\emph{Average Precision, AP}.
\begin{equation}
    AP = \int_{0}^{1} p(r)dr
\end{equation}
Ovvero l'integrale sotto la curva Precision-Recall $p(r)$. Questo è il parametro principale per valutare la qualità di un modello nel riconoscimento di una specifica classe, poichè valuta conteporanemante sia la Precision che la Recall. \\
Se il modello ammette più classi, si calcola l'AP per ogni classe separatemnte e si fa la media del risultato, ottenendo la $mAP$.
\begin{equation}
    mAP = \frac{1}{n}\sum_{i=1}^{n}AP_{i}
\end{equation}
Dove $n$ è il numero di classi e $AP_{i}$ è l'integrale dalla curva Precision-Recall dell'$i$-esima classe.
Per avere un modello coerente bisuogna definire una metodologia per discriminare i True Positive dai False Positive in base alla posizione della bounding box disegnata dal modello rispetto a quella di label. \\
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{precisionRecall2.png}
\caption{Curve Precision-Recall di vari modelli}
\end{figure}
\newpage
Per fare ciò si introduce il concetto di Interesection over Union (IoU). Ovvero, per ogni predizione del modello si calcola il rapporto tra l'intersezione delle due bounding boxes e la loro unione. \\
Quindi si scelgie un valore soglia arbitrario, a seconda delle esigenze del problema, al di sotto del quale la predizione del modello viene giudicata come non abbastanza precisa e quindi sbagliata. 
\\ Solo le predizioni del modello che soddifano il valore di soglia minimo della IoU vengono categorizzate come corrette e quindi concorrono positivamente al numero dei True Positive e quindi alla curva Precision.
\begin{figure}[h]
\centering
% \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{IoU.png}
\includegraphics[width=\linewidth]{IoU.png}
\caption{Interesection over Union}
\end{figure}
Nella prossima sezione verrà analizzato il funzionamento e l'archiettura di YOLO, un noto modello allo stato dell'arte nella Object Detection, utilizzato nella sua ultima iterazione nel progetto su cui si basa questa tesi.
\newpage
\section{YOLO: You Look Only Once}
Il modello YOLO (You Look Only Once) è di fatto una delle soluzioni più popolari e utilizzate nelle applicazioni di Object Detection. In tutte le sue versioni ha avuto numerosissimi usi nei domini applicativi più diversi, come per esempio la guida autonoma grazie alla sua velocità di inferenza fondamentale nelle applicazioni real time. E' stato inoltre utilizzato nell'argricoltura, videosorveglianza, medicina, monitoraggio del traffico e molto altro.\\
In generale è stato sempre riconosciuto dalla comunità come uno strumento particolarmente equilibrato tra velocità di inferenza e precisione.\\
Con l'avanzare delle versioni sono state fatte modifiche di design della rete, modifiche alla funzione di costo, modifiche alle anchor box e molto altro. 
\\Nel caso applicativo del progetto svolto in questa tesi si è usato l'ultima versione YOLOv8, la più performante. In questo elaborato ci si è dedicati dapprima alle generalità e ai principi fondamentali su cui si basa il modello, quindi all'analisi dell'evoluzione di YOLO partendo dall'architettura della prima versione e successivamente analizzando le modifiche e le aggiunte fondamentali delle successive.
\subsection{Funzionamento}
Come indicato dal nome, la rete propone un approccio a singolo stadio per il riconosicmento delle immagini. Molti dei modelli tradizionali di Object Detection precedenti a YOLO utilizzano l'approccio a \emph{sliding window}, ovvero scannerizzano l'immagine facendo scorrere una finestra e a ogni nuova posizione utilizzano un classificatore per controllare se all'interno della finestra sia presente effettivamente un'oggetto. 
Questa tecnica, per quanto intuitiva è profondamente inefficente in quanto per una sola immagina di input si utilizza un classificatore molte volte.
In generale comunque esistono una moltitudine di tecniche anche più avanzate che utilizzano comunque più stadi, dove per esempio il primo identifica le zone di interesse dell'immagine e successivamente un secondo stadio classifica il contenuto all'interno di tali zone. 
YOLO al contrario produce l'output con una sola regressione producendo in un solo passaggio sia le cordinate degli oggetti che le propabilità delle classi delle figure trovate tutto nello stesso output.
Diferrente dall'approccio sliding window è per quello di \emph{Fast R-CNN}[R-CNN] che produce due output separati generati rispettivamente da un classificatore per la probabilità della classe e un regressore per la posizione della bounding box. In particolare R-CNN generara una serie di potenziali bounding boxes nell'immagine (region proposals), quindi inferisce un classificatore su ognuna di esse e infine utilizza una serie di tecniche a posteriori per eliminare bounding boxes doppie e ricalcolare le metriche delle singole predizioni in base alla posizione delle altre. Ognuno di questi metodi deve essere addestrato separatamente e una singola inferenza attarversa varie interezioni.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{RCNN.png}
\caption{Funzionamento di R-CNN}
\end{figure}
Un'altro vantaggio di YOLO rispetto a R-CNN è quello di risucire a vedere tutta l'immagine durante il singolo step di inferenza, quindi considerando tutto il contesto e non solo i contesti delle singole region proposals. Questo porta a un notevole calo dei detect nelle zone di sfondo.

Per fare questo YOLO divide l'immagine di input in una griglia $S\times S$ e per ogni cella della griglia produce un numero $B$ di bounding boxes e i rispettivi punteggi di confidenza rispetto alle $C$ classi presenti nel problema per ognuna delle predizioni effettuate.
Nella fase finale solo le bounding boxes con confidenza al di sopra della soglia minima vengono effettivamente mostrate.

Per ogni boudning boxes YOLO produce 5 valori: $Pc,bx,by,bh,bw$, dove $Pc$ è detto \emph{confidence score} ovvero la confidenza del modello sulla possibiità che all'interno della regione sia effettivamente presente un oggetto, $bx$ e $by$ sono le coordinate del centro dell'oggetto relative alla cella della griglia. Infine $bh$ e $bw$ sono l'altezza e la larghezza della bounding box relativa all'intera immagine.
Per eliminare le bounding box duplicate sovrapposte tra di loro si utilizza la tecnica \emph{non-max supression} che rimuove le bounding box con il confidence score più basso quando la IoU delle due predizioni con la stessa classificazione è superiore a una certa soglia (solitamente 0.3). 

L'output di YOLO è quindi un tensore di grandezza $S \times S \times (B \times 5 + C)$
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{yoloOUTPUT.png}
\caption{Output di YOLO in un'immagine divisa con una griglia 3x3}
\end{figure}

\subsection{Architettura}
L'archiettettura del modello orginale YOLO è composta da 24 layer convoluzionali seguiti da due layer fully-connected che producono l'output delle coordinate delle bounding boxes e la classificazione degli oggetti. 
Tutti i layer utilizzano la funzione di attivazione chiamata \emph{leaky rectified linear unit}, la quale lascia invariati i valori positivi e divide per 10 quelli negativi. Mentre l'ultimo layer fully connected utilizza una funzione di attivazione lineare.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{yolo1_arch.png}
\caption{Architettura YOLOv1}
\end{figure}
Con il tempo e il passare delle iterazioni YOLO ha ottenuto enormi sviluppi e modifiche architetturali. Qui ci limitiamo a citare le modifiche più importanti apportate con il passare delle versioni. 

A partire da YOLOv2 sono state introdotte le cosiddette \emph{anchor boxes}, ovvero delle bounding boxes di dimensioni predefinite preparate a priori per ogni cella della grglia. Queste boxes sono pensate per avere forme prototipate compatibili con le classi che si stanno cercando. L'output del modello, ovvero il numero di predizioni totali sarà pari al numero di anchor boxes per cella moltiplicato per il numero di celle.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{anchor.png}
\caption{Anchor boxes introdotte in YOLOv2}
\end{figure}
A partire da YOLOv3 inizia a delinearsi una struttura che marcherà la composizione di tutte le versioni successive, ovvero la scomposizione logica del modello in tre moduli separati con compiti diversi: Backbone, Neck, Head.
Il \emph{Backbone} è la prima parte del modello, quella di più basso livello. Questo modulo è tipicamente una rete puramente convoluzionale incaricata di estrapolare feature grafiche dall'immagine iniziale.
Il \emph{Neck} ha lo scopo di collegare la parte finale (Head) del modello con il Backbone. Questo modulo aggrega e affina le features passategli dal backbone enfatizzando tratti sementici e spaziali di più alto livello rispetto a quelli estrapolati dal Backbone. 
Il modulo finale detto \emph{Head} si occupa di inferire le predizioni finali del modello sulla base delle features estratte dal Backbone e affinate dal Neck. E' composto da delle sottoreti incaricate di fare clasificazione e localizzazione degli oggetti. Inoltre sempre nell'Head sono presenti componenti in grado di eliminare bounding boxes duplicate e altre operazioni post predizione.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{backbone_neck_head.png}
\caption{Archietettura con Backbone, Neck e Head}
\end{figure}
A partire da YOLOv4 le aggiunte al modello sono state divise in due distinte categorie, \emph{bag-of-freebies} e \emph{bag-of-specials}.
Le prime sono modifiche principalemente relative alle modalità di training, come per esempio la \emph{data augmentation}, ovvero la generazione di ulteriori immagini di imput facendo modifiche a quelle già utilizzate. Questa classe di modifiche ha un impatto negativo solo sul tempo di training e non su quello di inferenza. 
Un'interessante esempio di aggiunta di tipo \emph{bag-of-freebies} in YOLOv4 è stato l'inserimento di Algoritmi Genetici per l'ottimizzazione degli iperparametri di training.
Le aggiunte categorizzate come \emph{bag-of-specials} sono invece tipicamente modifiche strutturali che incidono sul tempo di inferenza migliorando però la precione finale.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{bagofreebies.png}
\caption{Lista di aggiunte e modifiche su YOLOv4 divise fra Bag-of-Freebies and Bag-of-Specials}
\end{figure}
Con YOLOv7 sono state introdotte modifiche architetturali piuttosto sofisticate come le \emph{Extended efficient layer aggregation network (E-ELAN)}, ovvero reti che si basano su metodi capaci di fare convergere più velocemente un generico modello di Deep Learning controllando e ottimizzando il percorso di discesa del gradiente. Come si vede nella figura sotto, questo viene ottenuto posizionando dei collegamenti diretti fra layer lontani.\cite{elan}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{elan.png}
\caption{Efficient layer aggregation networks}
\end{figure}
\newpage
Con YOLOv8 è stato fasso un grosso passo in avanti per quanto riguarda la facilità d'uso.
E' infatti possibile utilizzare il modello da un'interfaccia a riga di comando (CLI) oppure istallandolo nel propio ambiente di sviluppo Python come un pacchetto PIP. 
YOLOv8 viene rilascaito in 5 versioni differenti con diversi tradeoff tra performance e precisione.
Valutato sul dataset di riferimento COCO 2017 YOLOv8x ha raggiunto una AP di 53.9\% con immagini di 640 pixels rispetto al 50.7\% di YOLOv5 con una velocità di 280 immagini per secondo su una GPU NVIDIA A100. 

\newpage
\section{Applicazione di YOLOv8 a campioni di nanografene}
Durante il tirocinio curricolare presso CNR-ISMN si è andati a sviluppare un modello predittivo basato su metodi di Computer vision per inferire propietà fisiche di campioni di nanografene simulati al calcolatore. 
Il primo modulo del progetto prevedeva di fare Object Detection sui campioni per individuare automaticamente i difetti all'interno della struttura atomica, ovvero le zone vuote, prive di atomi. 
Per fare questo si è scelto YOLO, per la sua facilità di utilizzo e di training, oltre alle sue elevate performance. In particolare è stato scelto YOLOv8, in quanto facilmente integrabile in un progetto Python essendo installabile tramite PIP. 

Pima di iniziare a lavorare sul training del modello è stato necessario procurarsi le immagini dei campioni, il dataset di grafene difettato a disposizione infatti non era sotto forma di immagini ma in formato \texttt{.xyz}. I file \texttt{.xyz} sono semplici file di testo pensati per rappresentare in modo preciso strutture di atomi nello spazio, la prima riga rappresenta il nome del campione, nella seconda riga è presente un semplice intero che rappresenta il numero di atomi presenti nel campione stesso, infine ciascuna dele righe successive descrivono la posizione di un atomo, indicando prima il tipo di atomo con una lettera (es. C = carbonio, H = idrogeno ecc...) e successivamente divisi da spazi le coordinati spaziali cartesiane del suddetto atomo, da questo deriva il nome del formato \texttt{.xyz}. Il dataset fornice anche un ulteriore file, dove è presente la lista per ogni campione del suo parametro fisico \texttt{total energy}, ovvero l'energia totale della struttura atomica del campione. Questa propietà è stata calcolata a priori durante la simulazone dei campioni al calcolatore e sarà proprio il parametro \emph{target} del predittore sviluppato nel progetto di cui parleremo nei prossimi capitoli. \newpage
Un'esempio di un campione di grafene difettato in formato \texttt{.xyz} utilizzato:

% \begin{lstlisting}
% 303
% Graphene
% C 0.01648 0.02603 0.01295
% C 12.34108 11.33991 0.01452
% ...
% ...
% C 30.70885 31.91716 0.04266
% C 2.44882 32.61991 0.01291
% \end{lstlisting}    

E' stato quindi necessario trasformare i campioni in formato \texttt{.xyz} in immagini, mantenendo il contenuto informativo realativo alla locazione e morfologia dei difetti presenti in esso. 
Per fare questo è stato scritto uno script in Python dove sono stati tracciati i legami tra atomi e non solo la posizine degli atomi stessi. In questo modo è molto più facile individuare e riconoscere la divisione tra parti difettate e parti piene. 
Nell'immagine successiva è presente a sinistra un render di un campione in formato \texttt{.xyz} utilizzando il software \texttt{VMD}, a destra invece il risultato prodotto dallo script, ovvero l'immagine.
\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{graphene_67.png}

(a) Render del campione
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{graphene_67_bonds.png}

(b)  Immagine prodotta
\end{minipage}
\caption{Comparazione fra Render del campione in formato \texttt{.xyz} e mmagine prodotta dallo script }
\end{figure}
A questo punto avendo i campioni pronti è stato fatto il labeling dei difetti utilizzando il software \texttt{labelimg} su 100 campioni. 
Il training quindi è stato fatto sulla piattaforma di calcolo cloud Google Colab, questa soluzione online permette di utilizzare gratuitamente una GPU NVIDIA Tesla T4 con 16GB di memoria.
Le 100 immagini annotate sono state divise in 80 per la fase di addestramento e 20 per la fase di test. 
Dopo 100 epoche i risultati sono stati abbastanza soddisfacenti per le specifiche e le necessità del progetto. 
\begin{figure}[h]
\centering
\includegraphics[width=350px,keepaspectratio]{outputYOLOreduced.jpg}
\caption{Risultati di YOLO su immagini di test con training effettuato su 80 immagini}
\end{figure}
\newpage
Il modello individua con una buona accuratezza i difetti, sopratutto quelli circolari. Fa più difficoltà invece con quelli di forma più oblunga, probabilmente perchè questi ultimi sono presenti in minor numero nelle immagini di training. 
Con un numero maggiore di immagini di training e il lavoro di labeling annesso sarebbe possibile affinare di molto l'addestramento della rete e ottenere risultati migliori. \\

%------------------------------------------------------------------------------------------------------------- CAPITOLO 3 -----------------------------------------------------------------------s--------------------------
{\let\clearpage\relax \chapter{Analisi dei campioni e sviluppo modello predittivo}}
In questo capitolo si descriveranno le tecniche utilizzate per l'analisi topologica dei difetti individuati da YOLO e le metodologie utilizzate per mettere in correlazione tali dati con i parametri fisici dei campioni originari.
Le informazioni raccolte saranno poi utilizzate come dati di addestramento del modello predittivo successivamente sviluppato. 
Si descriveranno inoltre i fondamenti teorici dei regressori statistici focalizzandosi sulla tipologia utilizzata nel modello predittivo sviluppato. 
Infine si analizzerà il processo di addestramento del modello predittivo discutendone i risultati. 
\section{Analisi dei campioni con OpenCV e correlazione risultati}
L'intuizione di base a questo punto del progetto è quella di indagare sull'eventuale presenza di una correlazione tra la morfologia geometrica dei difetti e le propietà fisiche dei campioni di grafene da cui questi derivano. 
In particolare come accennato precedentemente, i campioni di grafene utilizzati provengono da simulazioni al calcolatore dalle quali è stata calcolata l'energia totale del sistema. Questa importante propietà fisica fornisce un'indicazione dell'energia immagazinata nel sistema atomico. In particolare l'energia totale è la somma dell'energia dei singoli atomi di carbonio meno l'energia di formazione, ovvero l'energia che il sistema perde ddurante l'aggregazione degli atomi. Il totale è quindi inferiore all'energia dei sisngoli atomi presi singolarmente a grande distanza fra di loro. 
\begin{equation}
    E_{totale} = N_{atomi} * E_{carbonio} - E_{formazione} 
\end{equation}
Da questa relazione possiamo aspettarci livelli di energia totale più bassi per sistemi con pochi difetti e di conseguenza maggiore stabilità del sistema. Viceversa per campioni con grandi aree difettate ci aspettiamo livelli di energia più alti e in generale più instabilità.
% Questi ragionamenti sono veri in principio, ma occorre prestare attenzione al fatto che la maggior parte del contributo al valore dell'energia totale è dovuto all'energia degli atomi stessi. Quindi sarà necessario normalizzare i valori ottenuti per il numero di atomi presenti nei campioni per ottenere informazioni significative.  

\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{stabile.png}

(a) Struttura più stabile
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{meno_stabile.png}

(b)  Struttura meno stabile 
\end{minipage}
\caption{Comparazione strutture con diversi livelli di stabilità e quindi di energia totale}
\end{figure}
Il valore dell'energia totale sarà il parametro target del modello predittivo discusso più avanti. Lo scopo dell'intero progetto è quindi quello di predire il valore della total energy di un campione partendo dalla sua immagine.  

Per analizzare i difetti individuati si è andati ad utilizzare una nota libreria di computer vision, OpenCV. Questa libreria offre numerosi metodi per sconotornare ed estrapolare parametri numerici sulla base delle caratteristiche geometriche dell'immagine. 
Per prima cosa si è andati ad evidenziare le aree di interesse, ovvero i difetti nei campioni. In particolare le zone dove sono presenti i difetti saranno totalmente bianche mentre lo sfondo sarà nero. 
Per fare questo si è fatto uso della funzione \emph{findContours} di OpenCV, questo metodo è in grado di individuare e tracciare contorni di oggetti su immagini.
Una volta individuati i contorni è bastato selezionare quello con il perimetro maggiore per selezionare il difetto cercato ignorando tutto il resto. 
\begin{verbatim}
cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)    
\end{verbatim}
\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{graphene_67_bonds_cropped_box_1.png}

(a) Crop di YOLO
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{graphene_67_bonds_cropped_box_1_thresh_.png}

(b)  Crop di YOLO dopo la trasformazione 
\end{minipage}
\caption{Enfatizzazione delle aree difettate}
\end{figure}

In questo modo stiamo evidenziando le zone di interesse eliminando tutte le altre cavità nel reticolo atomico che non devono essere considerate come difetti.
A questo punto ci ritroviamo con il contorno che individua il difetto nell'immagine come mostrato nella figura sottostante.

\begin{figure}[h]
\centering
\includegraphics[width=200px,keepaspectratio]{graphene_67_bonds_cropped_box_1_thresh_countour_.png}
\caption{Contorno del difetto trovato con OpenCV}
\end{figure}
Il prossimo passo è stato quello di estrapolare i tratti geometrici del difetto stesso, sempre tramite metodi offerti da OpenCV. Ne sono stati estratti numerosi, dai più semplici come perimetro e area fino a proprietà più sofisticate (circolarità, solidità, compattezza...).
In generale non è detto che tutte queste propietà saranno indispensabili per il modello predittivo finale, il lavoro da fare a questo punto sarà propio quello di individuare le relazioni causali tra una propietà e l'altra oltre che con il paraemtro target (energia totale) con lo scopo di individaure i tratti più significativi sui quali successivamente fare il training del modello.

Per fare tutto ciò, i dati raccolti sono stati integrati in un dataframe \emph{Pandas}. Questa libreria offre metodi molto comodi per l'analisi di dati e sopratutto offre un oggetto chiamato dataframe adatto per ospitare i dati appena raccolti. 
Un dataframe Pandas si presenta che una tabella bidimensionale, simile a quelle di un foglio di calcolo, sulla quale si hanno a disposizione numerose funzioni per manipolare, confrontare e filtrare i dati. 

Prima di ottenere il dataframe finale sul quale sono state fatte le analisi è stato necessario risolvere un problema che ha a che fare con la natura delle immagini prodotte da YOLO. Quest'ultimo infatti per ogni difetto trovato in una delle immagini campione originarie produce in generale più di una immagine, una per ogni difetto trovato. L'analisi tramite OpenCV e le propietà estratte quindi sono relative in generale a un singolo difetto proveniente da un campione che ne può contenere diversi. Il parametro total energy invece fa riferimento all'intero campione originale e non ai singoli difetti estratti da esso. Questo crea un problema in fase di correlazione tra propietà dei singoli difetti e il  parametro total energy. 
Per risolvere questo problema si è deciso di collassare i parametri estrapoalti dai singoli difetti in una sola riga del dataframe facendo la media pesata dei valori delle varie proprietà per l'area dei difetti. 
Questa scelta è stata fatta partendo dal presupposto che difetti di area maggiore abbiano un'importanza più grande rispetto a quelli di grendezza inferiore. 

Il dataframe su cui verranno fatte le analisi è nella forma seguente, una riga per ogni campione originario, una colonna per ogni propietà estrapolata più il parametro total energy di riferimento per quel specifico campione. 

\begin{center}
\begin{tabular}{ |p{1cm}||p{1.5cm}|p{2.5cm}|p{1cm}|p{3cm}|  }
 \hline
 \multicolumn{5}{|c|}{Dataframe Pandas} \\
 \hline
 id & area\_px & periemtro\_px & ... & total\_energy \\
 \hline
 0 & 72 & 46 & ... & -3922\\
 1 & 923 & 144 & ... & -3048\\
 2 & 1326 & 284 & ... & -2599\\
 3 & ... & ... & ... & ...\\
 4 & ... & ... & ... & ...\\
 5 & ... & ... & ... & ...\\
 6 & ... & ... & ... & ...\\
 \hline
\end{tabular}    
\end{center}
Il primo grafico estratto dal dataframe è stata una heatmap, questo tipo di grafico è ideale per individuare in prima battuta le correlazioni più forti tra le propietà.
Queste correlazioni sono importanti, in quanto permettono di indivudare parametri molto dipendenti tra loro, spesso con correlazione direttamente o inversamente proporzionale. 
Di questi parametri quindi sarà sufficiente utilizzarne uno solo durante l'addestramento del modello predittivo, in quanto portano lo stesso contenuto informativo e quindi non è necessario utilizzarli entrambi. Questi tipi di ragionamenti nell'addestramento di modelli su dataset molto grandi può fare la differenza in quanto a tempo di addestramento.
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=400px,keepaspectratio]{heatmap.png}
\caption{Heatmap generata dal dataframe}
\end{figure}    
\end{center}
\newpage
Nel lato alto a destra delle heatmap si confermano delle dipenze intuibili a priori, per esempio l'area e il perimetro correlano quasi perfettamente. Questo è del tutto ragionevole in quanto in generale difetti con aree grandi avranno perimetri tentenzialmente grandi. 

Dopo la heatmap si è andati a generare un correlogramma che similmente alla heatmap mette in correlazione tutte le possibili coppie di properietà mostrandone questa volta anche l'andamento. 
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=400px,keepaspectratio]{corr.png}
\caption{Correlogramma }
\end{figure}    
\end{center}
I risultati estrapolabili da questo grafico sono molteplici. Si possono osservare correlazioni di vario tipo, dirette e inverse, lineari, logaritmiche, esponeniziali e anche andamenti più caotici dove non è chiaro se ci sia effettivamente una correlazione. 
Queste ultime pur se meno ovvie e interpretabili potranno dare comunque un contributo importante nell'addestramento del predittore. 

Alla luce del correlogramma ci si è concentrati nello studio delle ultime due righe, ovvero
quelle che mettono in correlazione i parametri dell’energia totale in valore assoluto e scalata per il numero di atomi con il resto delle proprietà estratte dai campioni.
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=200px,keepaspectratio]{energy_vs_area.png}
\caption{Zoom del correlogramma per osservare l’andamento della total energy
all’aumentare dell’area dei difetti}
\end{figure}    
\end{center}
Ciò che è davvero importante è la correlazione quasi lineare tra energia e area dei difetti (e corrispettivi parametri correlati strettamente all’area).
Questa correlazione emerge qualitativamente sia con l’energia assoluta che con quella
normalizzata per il numero di atomi, ciò significa che l’area dei difetti ha un chiaro effetto nell’aumentare l’energia del sistema e quindi la sua instabilità.
Alla luce di queste considerazioni sarà possibile successivamente selezionare con criterio le propietà su cui effettuare il training del modello predittivo. 
Nella sezione successiva si introdurrà la teoria dei regressori statistici, focalizzandosi sugli alberi decisionali, utilizzati per il modello. 
\section{Regressione Statistica}
Con regressione statistica si intendono tutti i metodi e i modelli matematici atti ad analizzare dati raccolti (variabili indipendenti) con lo scopo di ottenere predizioni su parametri cercati (variabili dipendenti). 
Lo scopo è quindi quello di trovare una relazione funzionale tra variabili indipendenti (osservazioni) e variabili dipendenti.
Queste funzioni possono essere semplici rette nel caso della \emph{Regressione lineare} o più in generale curve di vario tipo, più adatte a modellare fenomeni più complessi.
Queste predizioni possono derivare da dati di partenza di diverso tipo, come per esempio serie temporali, o come nel nostro caso propietà numeriche derivanti da analisi geoemtriche di immagini. L'operazione nella quale si ottiene una predizione su una variabile dipendente a partire dai dati raccolti di una o più variabili indipendenti prende il nome di inferenza statistica.
In questa sezione analizzaermo brevemente  il tipo più semplice di regessione statistica, ovvero quella lineare per poi introdurre modelli di natura diversa come gli alberi decisionali. 
\subsection{Regressione lineare}
Il caso più semplice di regressione è quella lineare singola. Lineare perchè la relazione funzionale che si andrà a cercare è una retta, singola perchè la variabile indipendente su cui si sono svolte le osservazioni è una sola. In particolare da una serie di osservazioni numeriche di una variabile indipendente si vuole tracciare una retta che descriva al meglio l'andamento dei dati osservati. Questa retta è proprio la funzione da campionare per ottenere le predizioni richieste. 
La funzione cercata si presenta come una semplice retta del tipo: 
\begin{equation}
    y = \beta_{0} + \beta_{1}x
\end{equation}
Dove, 
\begin{itemize}
    \item y: è il valore stimato (predizione)
    \item $\beta_{0}$ è l'intercetta della retta di regressione, ovvero il valore di y nell'origine. 
    \item $\beta_{1}$ è il coefficente angolare della retta
    \item x: è la variabile indipendente 
\end{itemize}
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{Normdist_regression.png}
\caption{Retta di regressione (in blu) e osservazioni (in rosso). }
\end{figure}    
\end{center}
I due paramtri $\beta_{0}$, $\beta_{1}$ sono quelli che il modello matematico dovrà ottimizzare con lo scopo di ottenere la migliore relazione possibile. 
Per fare ciò esistono vari metodi, il più famoso e utilizzato prende il nome di \emph{Metodo dei Minimi Quadrati (RSS, Residual Sum of Squares)}. Questo metodo permette di trovare i parametri che minimizzino il più possibile la somma dei quadrati delle distanze tra i dati osservati (valori di $x$) e quelli della curva (in questo caso la retta $\beta_{0} + \beta_{1}x$).
Più precisamente si volgiono trovare i parametri $\beta_{0}$, $\beta_{1}$ tali che si minimizzi la funzione: 
\begin{equation}
    \min{RSS} = \min{\sum_{i}(y_{i} - \hat{y_{i}})^2} = \min{\sum_{i}(y_{i}-(\beta_{0} + \beta_{1}x_{i}))^2}
\end{equation}
Dove, 
\begin{itemize}
    \item $y_{i}$ sono i valori reali (valori attesi)
    \item $\hat{y_{i}}$ sono i valori stimati
    \item $\sum_{i}(y_{i} - \hat{y_{i}})$ è quindi l'errore sulla $i$-eesima stima, (residuo)
\end{itemize}
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{residual.png}
\caption{I residui sono la distanza tra la curva di regressione e le osservazioni effettive. }
\end{figure}    
\end{center}
Se calcoliamo la media aritmetica di x e y e la definiamo come:
    \begin{equation}
        \Bar{x} = \frac{1}{n} \sum_{i}x_{i} \;\; \Bar{y} = \frac{1}{n} \sum_{i}y_{i}
    \end{equation}
Possiamo scrivere i parametri cercati come: 
\begin{equation}
    \beta_{0} = \Bar{y} - \beta_{1}\Bar{x}
\end{equation}
\begin{equation}
    \beta_{1} = \frac{\sum_{i}(x_{i}-\Bar{x})(y_{i}-\Bar{y})}{\sum_{i}(x_{i}-\Bar{x})^2}
\end{equation}

Per valutare un modello regressivo si utilizza un parametro chiamato $R^2$ che da un'indicazione su come la curva approssima l'andamento reale delle osservazioni. 
\begin{equation}
    R^2 = \frac{TSS - RSS}{TSS} = 1- \frac{RSS}{TSS}
\end{equation}
Dove, RSS è la somma del quadrato dei residui
\begin{equation}
    RSS = \sum_{i}(y_{i} - \hat{y_{i}})^2
\end{equation}
e TSS è la somma totale dei quadrati della differenza tra le predizioni e la loro media.
\begin{equation}
        TSS = \sum_{i}(y_{i} - \Bar{y_{i}})^2 \; \;
    \Bar{y} = \frac{1}{n} \sum_{i}y_{i}
\end{equation}
Il parametro $R^2$ varia tra 0 e 1 ed è sintomo di una buona performace del modello se è prossimo a 1. 

Questo metodo è estendibile a problemi con più variabili indipendenti e più in generale a regessioni polinomiali, in cui la relazione può assumere un andamento non lineare.

\subsection{Alberi decisionali}
Nel mondo del machine learning supervisionato esiste un altro approccio diverso da quelli analizzati in precedenza, ovvero gli alberi decisonali. 
Similmente a ciò che abbiamo appena discusso con i regressori lineari, gli alberi decisionali mirano a dare delle predizione partendo da un insieme di osservazioni raccolte. Queste predizioni possono essere relative a valori target discreti (classificazione) oppure continui (regressione). Data la natura dell'applicazione di questo elaborato ci concentreremo sul secondo tipo. 
Gli alberi sono strutture gerarchiche formate da nodi, da ognuno dei quali partono due nodi figli fino ad arrivare alle cosiddette \emph{foglie}, ovvero nodi che non hanno figli. Ogni nodo in realtà è una funzione binaria di una variabile dal cui risulato dipende il percorso da prendere. 
Il modello consiste quindi in una serie di domande sulle variabili osservate con le quali si discende l'albero dall'alto verso il basso fino a giungere una foglia che rappresenta la predizione stessa del modello. 
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{DecisionTree_Classification_Graphic.png}
\caption{Semplice esempio di albero decisionale per classificazione binaria con due variabili indipendenti $x_{1}$, $x_{2}$  }
\end{figure}    
\end{center}
I dati vengono forniti nel seguente modo, 
\begin{equation}
    (\bold{x}, Y) = (x_{1}, x_{2}, ... , x_{k}, Y) 
\end{equation}
Dove $x_{1}, x_{2}, ... , x_{k}$ sono i valori delle variabili indipendeti per l'osserrvazione dove il parametro target vale $Y$.

Gli alberi decisonali vantano una grande intelligibilità diversamente da altri metodologie di apprendimento automatico, in quanto è possibile indagare sul funzionamento del modello e dell'esito della singola predizione. E' facile infatti andare a ritroso per capire e giustificare la predizione del modello, analizzando il percorso fatto nella discesa della struttura ad albero. 
Un altro punto a favore nell'uso degli alberi decisonali è la capacità di performare una selezione delle fetaure. Durante le epoche di addestramento infatti alcune feature (i loro nodi) possono essere rimosse o spostate verso il basso in quanto reputate scorrelate dal parametro target cercato. 
E' importante ricordare che gli alberi sono strutture strettamente \emph{gerarchiche}, ovvero le feature analizzate nei nodi più in alto sono reputate più importanti rispetto a quelle analizzate più in basso e hanno un incidenza maggiore.
Per quanto riguarda i difetti di questo approccio, il principale problema è la tendenza ad andare in overfitting, per evitare questo si usa una tecnica chiamata \emph{Pruning}, che consiste nella rimozione di parti della satruttura ad albero considerate non critiche, semplificando e ottimizzando il modello senza rinunciare alla precisione delle predizioni. 
\section{Ensemble learning}
In questa sezione estendiamo le fondamenta degli alberi decisionali appena discusse a una gamma di metodi che prende il nome di \emph{Ensable learning}.
L'idea di base è quella di utilizzare più di un albero decisonale per ottenere la predizione finale. Ci si aspetta che addestrando più alberi su sottoinsiemi casuali del dataset e delle feature di partenza si possano ottenere performance più elevate rispetto a quelle del singolo albero. Inoltre, addestrando gli alberi su sottoinsiemi casuali del dataset di partenza si va a mitigare il problema di overfitting discusso in precedenza.
Analizzeremo i due approcci principali, Bagging e Boosting.
\subsection{Bagging e Random Forest}
L'idea base del \emph{Boostrap aggregating} o \emph{Bagging} è quella di creare a partire dal dataset di partenza più dataset della stessa dimensione, scegliendo casualemente elementi dal dataset principale ammettendo ripetizioni e a partire da questi nuovi dataset addestrare strutture ad albero in parallelo. 

Formalmente se abbiamo un training set $X = x_{1}, ... , x_{n}$ con le rispettive risposte $Y = y_{1}, ... , y_{n}$ andiamo a creare un numero $B$ di nuovi insiemi pescando casualemte $n$ elementi dal dataset iniziale con ripetizione.

Quindi per $b = 1, ..., B$
\begin{enumerate}
    \item Estraiamo casualmente da $(X,Y)$ $n$ coppie $(x,y)$ con ripetizione, formando un nuovo dataset della stessa grandezza chiamato $(X_{b}, Y_{b})$. 
    \item Addestraiamo un albero di classificazione o regressione chiamto $f_{b}$ sui dati $(X_{b}, Y_{b})$.
\end{enumerate}

A questo punto ci si ritrova con un numero $B$ di alberi addestrati su dataset non identici, dai quali si ottiene la predizione finale per un certo valore di input $x'$ facendo la media delle predizioni dei singoli alberi (nel caso della regressione).
\begin{equation}
    f(x') = \frac{1}{B}\sum_{b=1}^{B} f_{b}(x')
\end{equation}
Questa metodologia prende il nome di Bagging e può essere estesa nei modelli chiamati \emph{Random Forest} aggiungendo un'ulteriore casualità nell'apprendimento. In particolare si utilizza un algoritmo di apprendimento che per ogni sottoalbero seleziona un sottoinsieme delle feature di partenza, in questo modo i singoli alberi si \emph{specializzano} nelle feature di loro competenza.
In generale gli algoritmi \emph{Random Forest} superano in prestazioni gli alberi decisionali singoli, avendo meno overfitting ma perdono notevolmente di intelligibilità in quanto è molto più difficile andare a ritroso e interpretare una predizione. 
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{random forest.png}
\caption{Random Forest}
\end{figure}    
\end{center}
\subsection{Gradient Tree Boosting }
Analizziamo ora un modello più sofisticato e molto usato nelle applicazioni e che ha dimostrato di sorpassare le performance dei modelli \emph{Random Forest}.
Il \emph{Gradient Tree Boosting} è una metologia che similmente al meotodo \emph{Random Forest} addestra molteplici predittori (alberi decisonali) per ottenere una predizione complessiva più accurata. Differentemente dal caso appena analizzato gli alberi non vengono addestrati in parallelo (conteporaneamente) ma in serie. Da ogni albero ne viene prodotto uno nuovo che va a migliorare quello precedente in modo iterativo. Visto la natura molto tecnica dell'argomento non entreremo nei dettagli dell'algoritmo di discesa del gradiente utilizzato per costrutire gli alberi a partire da quelli precedenti ma ci limiteremo a una visione ad alto livello del metodo. 

L'obbiettivo è quello di ottenere un predittore (albero) $F$ in grado di predire valori nella forma di $\hat{y} = F(x)$ minimizzando l'errore quadratico medio (MSE) tra valori predetti e valori reali, $\frac{1}{n}\sum_{i} (\hat{y_{i}} - y_{i})^2$, dove: 
\begin{itemize}
    \item $\hat{y_{i}}$ sono i valori predetti 
    \item $y_{i}$ sono i valori reali 
    \item $n$ è il numero di osservazioni del dataset 
\end{itemize}
Utilizzando un algoritmo di Gradient Boosting (che non approfondiremo) avremo per ognuna delle M iterazioni
un nuovo albero $F_{m+1}$ creato a partire da quello precedente $F_{m+1}$ andando ad ottimizzare la funzione di costo MSE: 
\begin{equation}
    F_{m+1}(x_{i}) = F_{m}(x_{i}) + h_{m}(x_{i}) = y_{i}
\end{equation}
Dove $h_{m}(x_{i})$ (estimatore) è l'aggiunta apportata dall'algoritmo durante lo step.
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=350px,keepaspectratio]{gradient tree boosting.png}
\caption{Gradient Tree Boosting   }
\end{figure}    
\end{center}
In generale quindi la predizione finale verrà scelta dando un peso diverso a ogni classificatore, favorendo quelli con precisione migliore in fase di training. 


Nella prossima sezione si discutera l'applicazione di una implementazione di questo meotodo sul problema applicativo affrontato durante questo eleborato, ovvero la costruzione di un modello in grado di predirre il parametro totale energy a partire da una serie di features geomtriche. 


\section{Sviluppo modello predittivo con regressore XGBoost}
Alla luce di tutto ciò che è stato discusso fino ad ora possiamo discutere l'applicazione di una implementazione dell'algoritmo di Gradient Boosting al problema iniziale di questo elaborato, ovvero addestrare un predittore in grado di predire il paramentro total energy partendo dalle feature geomtriche estratte dai campioni. 

In particolare è stata scelta l'implementazione XGBoost offerta dalla libreria Python open source \emph{scikit-learn}, che offre numerosissime funzioni e algoritmi di machine learning.

In particolare è stata scelta la funzione: 
\begin{verbatim}
    sklearn.ensemble.GradientBoostingRegressor
\end{verbatim}

Le feature scelte per il training come discusso in precedenza sono state selezionate con l'intenzione di scegliere solo propietà scorrelate tra di loro, in quanto sarebbe ridondante effettuare il training su fetaure che correllano fortemente tra di loro e che quindi non portano contenuto informativo aggiuntivo. 
In particolare sono state scelte: 
\begin{verbatim}
    X = (area_px, num_pixels, circularity, solidity,
    compactness, feret_diameter, eccentricity)
\end{verbatim}
A questo punto si è preso l’array dei valori delle energie corrispondenti che il modello userà come parametro target. La funzione di costo da minimizzare si baserà su questo parametro. 
\begin{verbatim}
    y = (total_energy)
\end{verbatim}
Per ogni tupla X di feature associamo il valore corrispondente dell'energia di y.

Il dataset è stato diviso al 90/10 su train e test.
\begin{verbatim}
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.1, random_state=42
    )  
\end{verbatim}
Gli iperparametri scelti sono stati i seguenti:
\begin{verbatim}
    params = {
        "n_estimators": 500,
        "max_depth": 20,
        "min_samples_split": 5,
        "learning_rate": 0.01,
        "loss": "squared_error",
    }
\end{verbatim}
\begin{itemize}
    \item 	\texttt{n\_estimators} è il numero di iterazioni e quindi di alberi utilizzati, è stata aumentata da 100 a 500 rispetto ai parametri default 
    \item \texttt{max\_depth} è la profondità dei singoli alberi, è stata aumentata da 3 a 20 rispetto ai parametri default 
    \item \texttt{min\_samples\_split} e \texttt{learning\_rate} sono stati lasciati a default
    \item Come funzione di costo è stato scelto lo scarto quadratico medio.
\end{itemize}

A questo punto viene chiamato il metodo \texttt{fit} per addestrare il modello, specificando il dataset di train: 
\begin{verbatim}
    reg = ensemble.GradientBoostingRegressor(**params)
    reg.fit(X_train, y_train)    
\end{verbatim}
Utilizzando 2000 campioni con 7 features, l’addestramento è molto breve, nell’ordine dei pochi secondi. 

Per verificare le performance del modello si valuta l'errore quadratico medio sul dataset di test: 
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{deviance.png}
\caption{Errore quadratico medio durante le epoche di addestramento   }
\end{figure}    
\end{center}
\begin{verbatim}
    mse = mean_squared_error(y_test, reg.predict(X_test))
    print(...)
    The mean squared error (MSE) on test set: 5262.5476
\end{verbatim}

La devianza è circa 5000, ovvero l’errore medio sulla predizione dell’energia del campione si attesta sulle decine. Considerando che il range di valori dell’energia è di circa 2000 unità il modello si dimostra piuttosto preciso. 
Il modello come spesso accade ha precisione maggiore nel dataset di train (blu) rispetto a quello di test (rosso), in quanto esiste sempre un certo livello di overfitting, la differenza però in questo caso non è drammatica. 

Dal grafico inoltre vediamo un calo dell'apprendimento dopo le 300 iterazioni. Si potrebbe quindi limitare le iterazioni dell’addestramento a quel valore.


E’ stato poi analizzato un istogramma chiamato (MDI) che suggerisce quali tra le feature hanno avuto un'importanza maggiore durante l'addestramento.
\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=250px,keepaspectratio]{mdi.png}
\caption{Grafico MDI per visualizzare l'importanza delle features}
\end{figure}    
\end{center}
Come intuibile a priori l’area dei difetti si dimostra la feature più significativa tra quelle presenti. Vedendo un’importanza tale si è fatto l'esperimento di ripetere l’addestramento utilizzando solo questa come singola feature. I risultati però sono stati scarsi, è quindi chiaro che anche le altre feature, prese insieme apportano un contributo fondamentale all’addestramento e quindi alla performance del modello. 
\newpage
Si è quindi osservata la curva di fit che evidenzia un’elevata precisione nella predizione di valori di energia bassi (difetti piccoli) e viceversa predizioni meno accurate su valori di energia alti (difetti grandi).

\begin{center}
\begin{figure}[h]
\centering
\includegraphics[width=300px,keepaspectratio]{fit.png}
\caption{Curva di fit}
\end{figure}    
\end{center} 

Si è andati quindi a fare delle predizioni a campione per verificare l'errore medio: 
\begin{verbatim}
Target total_energy: 
[    -2284.8     -2910.9     -2570.9     -3649.1     ...     -3536.9]
Predicted total_energy: 
[      -2283     -2910.7     -2542.8     -3644.4     ...    -3536.3]
Error: 
[    -1.7482    -0.21449     -28.075     -4.7009     ...    -0.63399]
\end{verbatim}

Si conferma un errore medio nell’ordine delle decine. 
\newpage
Infine si è andati a calcolare l'indice $R^2$, che si attesta su 0.97, molto prossimo a 1.  
\begin{verbatim}
    reg.score(X_test, y_test)
    0.9711437904543666
\end{verbatim}
Da questo valore possiamo concludere costatando che il modello è molto preciso nella predizione del valore cercato.
\subsection{Sviluppi futuri ed estensione ad immagini reali}
Questo progetto si è dimostrato adatto alla predizione della total energy su immagini derivanti da campioni simulati di nanografene. Questo lavoro può essere visto come un esperimento di fattibilità preliminare che pone le basi per una eventuale estensione a immagini reali. 
Infatti, se fosse possibile applicare le metodologie usate in questo elaborato su generiche immagini di microscopia di nanografene, tenendo conto degli opportuni valori di scala sarebbe possibile ottenere in linea teorica un prodotto interessante a livello industriale e applicabile sul campo, 

Come prevedibile questa generalizzazione non è banale e presenta numerose difficoltà in varie parti del progetto. 
La problematica più evidente si pone nel primo modulo del progetto, ovvero nel riconoscimento dei difetti tramite YOLO. Il suddetto modello infatti è stato addestrato su un dataset di grafene simulato e non su generiche immagini di microscopia. In particolare le immagini su cui è stato svolto l'addestramento sono molto omogenee, a bassa risulozione, tutte della stessa dimensione e scala. Inoltre sono state private al massimo del rumore grazie al lavoro di postprocessing applicato su di esse. Le imamgini di microsopia invece, possono essere fortemente eterogenee tra di loro, con molto più rumore, contenuto informativo più alto e scale diverse. In generale quindi sono molto differenti da quelle su cui YOLO è stato addestrato.

Il problema potrebbe essere risolto andando a ripetere l'addestramento per ogni dataset di microscopia che si vuole analizzare, con tutto il lavoro di labeling manuale che ne consegue. Questa soluzione è sicuramente poco attraente perchè rinuncia alla generalità dello strumento e alla sua appetibilità applicazione in scenari industriali,

Un'altro approccio, per superare questo problema potrebbe essere quello di cercare di trasformare le immagini di microscopia in copie il più possibili simili a quelle su cui è stato fatto il training di YOLO. 
Nella coppia di immagini sottostante viene mostrato uno di questi tentativi atto a ridurre il rumore e trasformare un'immagine reale in una versione il più possibile simile a quelle consociute da YOLO. Si è sempre utilizzato OpenCV per manipolare le immagini.  

\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{graphene_real_1.png}

(a) Immagine reale al miscoscopio di grafene 
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{graphene_real_1_red.png}

(b) Immagine processata 
\end{minipage}
\caption{Postprocessing di un'immagine reale di campione di grefene}
\end{figure}

Questa metodologia pur avendo dato risultati misti si dimostra promettente e lascia spazio per ulteriore lavoro ed estensioni future. 

\chapter*{Conclusioni}    
  \addcontentsline{toc}{chapter}{Conclusioni}

% \clearpage{\pagestyle{empty}\cleardoublepage}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%per fare le conclusioni
% \chapter*{Conclusioni}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries
% CONCLUSIONI}]{\fancyplain{}{\bfseries\thepage}}
% \lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
% CONCLUSIONI}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%aggiunge la voce Conclusioni
%                                         %   nell'indice
% \addcontentsline{toc}{chapter}{Conclusioni} Queste sono le
% conclusioni.\\
% In queste conclusioni voglio fare un riferimento alla
% bibliografia: questo \`e il mio riferimento \cite{K3,K4}.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \renewcommand{\chaptermark}[1]{\markright{\thechapter \ #1}{}}
% \lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries\rightmark}}
% \appendix                               %imposta le appendici
% \chapter{Prima Appendice}               %crea l'appendice
% In questa Appendice non si \`e utilizzato il comando:\\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\verb"" è equivalente all'
%                                         %   ambiente verbatim,
%                                         %   ma si utilizza all'interno
%                                         %   di un discorso.
% \verb"\clearpage{\pagestyle{empty}\cleardoublepage}", ed infatti
% l'ultima pagina 8 ha l'intestazione con il numero di pagina in
% alto.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries \thechapter \:Prima Appendice}]
% {\fancyplain{}{\bfseries\thepage}}
% \chapter{Seconda Appendice}             %crea l'appendice
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries \thechapter \:Seconda Appendice}]
% {\fancyplain{}{\bfseries\thepage}}
\begin{thebibliography}{90}             %crea l'ambiente bibliografia
\rhead[\fancyplain{}{\bfseries \leftmark}]{\fancyplain{}{\bfseries
\thepage}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%aggiunge la voce Bibliografia
                                        %   nell'indice
\addcontentsline{toc}{chapter}{Bibliografia}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%provare anche questo comando:
%%%%%%%%%%%\addcontentsline{toc}{chapter}{\numberline{}{Bibliografia}}
%-----------------------primo capitolo ------------------------
% textbook generale, molto completo sulle NN 
\bibitem{NN_book} R. Rojas, \emph{Neural Networks}, Springer-Verlag, Berlin, 1996
\bibitem{GD} Bottou, Léon; Bousquet, Olivier (2007). \emph{The Tradeoffs of Large Scale Learning}
%articolo sulla backpropagation
\bibitem{backprop} Annette Lopez Davila, Professor Chi-Kwong Li, \emph{Neural Networks: The Professor Chi-Kwong Li Algorithm} Math 400, College of William and Mary
%articolo sull'esempio del capitolo 1
\bibitem{handwritten} Ritik Dixit, Rishika Kushwah, Samay Pashine, \emph{Handwritten Digit Recognition using Machine and Deep Learning Algorithms}, Computer Science and Engineering
Acropolis Institute of Technology \& Research
Indore, India

%-----------------------secondo capitolo ------------------------
%CNN
\bibitem{cnn_overview}Rikiya Yamashita, Mizuho Nishio, Richard Kinh Gian Do Kaori Togashi \emph{Convolutional neural networks: an overview
and application in radiology}, Insights into Imaging volume 9, pages 611–629 (2018)
\bibitem{AP} Jesús María Romero Riveros, Prof. Piero Fraternali \emph{A metric evaluation framework for object detection}
\bibitem{yolo1}  Santosh Divvala, Ross Girshick, Ali Farhadi \emph{You Only Look Once: Unified, Real-Time Object Detection} University of Washington, Allen Institute for AI, Facebook AI Research

%articolo completo con storia, funzionamento e architettura di YOLO. Joseph Redmon∗
\bibitem{YOLO-story} Juan R. Terven, Diana M. Cordova-Esparaza, \emph{A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond}

\bibitem{YOLOv7_paper}Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao \emph{YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object
detectors}, Institute of Information Science, Academia Sinica, Taiwan

\bibitem{elan}Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh \emph{Designing Network Design Strategies Through Gradient Path Analysis}
Institute of Information Science, Academia Sinica, Taiwan Elan Microelectronics Corporation, Taiwan

\bibitem{regression} Tania Cerquitelli, Elena Baralis
\emph{Regression Analysis: Fundamentals}
Politecnico di Torino

\bibitem{ensemble} Lee, H.-L.; Kim, J.-S.; Hong,
C.-H.; Cho, D.-K. Ensemble Learning
Approach for the Prediction of
Quantitative Rock Damage Using
Various Acoustic Emission

\bibitem{random forest} Adele Cutler, David Richard Cutler, John R Stevens,
Ensemble Machine Learning: Methods and Applications (pp.157-176)

\bibitem{Gradient Boosting} Cheng Li,
A Gentle Introduction to Gradient Boosting,
College of Computer and Information Science
Northeastern University

\bibitem{XGBoost}Tianqi Chen, Carlos Guestrin, 
XGBoost: A Scalable Tree Boosting System, 
University of Washington

\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \chapter*{Ringraziamenti}
% \thispagestyle{empty}
% Qui possiamo ringraziare il mondo intero!!!!!!!!!!\\
% Ovviamente solo se uno vuole, non \`e obbligatorio.
\end{document}
