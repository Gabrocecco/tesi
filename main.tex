

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%12pt: grandezza carattere
                                        %a4paper: formato a4
                                        %openright: apre i capitoli a destra
                                        %twoside: serve per fare un
                                        %   documento fronteretro
                                        %report: stile tesi (oppure book)
\documentclass[12pt,a4paper,openright,twoside]{report}
\usepackage{tikz}
\usepackage{amsmath, xparse}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per scrivere in italiano
% \usepackage[italian]{babel}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per accettare i caratteri
                                        %   digitati da tastiera come è à
                                        %   si può usare anche
                                        %   \usepackage[T1]{fontenc}
                                        %   però con questa libreria
                                        %   il tempo di compilazione
                                        %   aumenta
% \usepackage[latin1]{inputenc}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per impostare il documento
\usepackage{fancyhdr}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per avere l'indentazione
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   all'inizio dei capitoli, ...
\usepackage{indentfirst}
%
%%%%%%%%%libreria per mostrare le etichette
%\usepackage{showkeys}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per inserire grafici
\usepackage{graphicx}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%libreria per utilizzare font
                                        %   particolari ad esempio
                                        %   \textsc{}
\usepackage{newlfont}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%librerie matematiche
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amsthm}
%
\oddsidemargin=30pt \evensidemargin=20pt%impostano i margini
\hyphenation{sil-la-ba-zio-ne pa-ren-te-si}%serve per la sillabazione: tra parentesi 
					   %vanno inserite come nell'esempio le parole 
%					   %che latex non riesce a tagliare nel modo giusto andando a capo.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%comandi per l'impostazione
                                        %   della pagina, vedi il manuale
                                        %   della libreria fancyhdr
                                        %   per ulteriori delucidazioni
\pagestyle{fancy}\addtolength{\headwidth}{20pt}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection \ #1}{}}
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\cfoot{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linespread{1.3}                        %comando per impostare l'interlinea
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%definisce nuovi comandi
%
\begin{document}
\begin{titlepage}                       %crea un ambiente libero da vincoli
                                        %   di margini e grandezza caratteri:
                                        %   si pu\`o modificare quello che si
                                        %   vuole, tanto fuori da questo
                                        %   ambiente tutto viene ristabilito
%
\thispagestyle{empty}                   %elimina il numero della pagina
\topmargin=6.5cm                        %imposta il margina superiore a 6.5cm
\raggedleft                             %incolonna la scrittura a destra
\large                                  %aumenta la grandezza del carattere
                                        %   a 14pt
%\em                                     %emfatizza (corsivo) il carattere
%Questa \`e la \textsc{Dedica}:\\
%ognuno pu\`o scrivere quello che vuole, \\
%anche nulla \ldots                      %\ldots lascia tre puntini
\newpage                                %va in una pagina nuova
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage{\pagestyle{empty}\cleardoublepage}%non numera l'ultima pagina sinistra
\end{titlepage}
% \pagenumbering{roman}    %serve per mettere i numeri romani

\tableofcontents
\begin{abstract}
    Il tema su cui verte questa tesi sono le attività svolte durante il tirocinio curricolare presso \emph{CNR-ISMN (Istitituto per lo Studio dei Materiali Nanostrutturati)}. \\
    L'obbiettivo del progetto svolto è stato quello di mettere in correllazione le propietà chimico/ficiche di campioni di grafene, provenienti da simulazioni al calcolatore, con
    i \emph{difetti} e \emph{imperfezioni} presenti nella loro struttura atomica.\\
    I sudetti \emph{difetti} infatti si presentano come dei \emph{vuoti} all'interno della struttura atomica, queste \emph{cavità} quindi sono state individuate e analizzate automaticamente utilizzando vari strumenti, fra cui la rete neurale di Object Detection \emph{YOLOv8} e la libreria di Computer Vision \emph{OpenCV}.
    Grazie a questi metodi è stato possibile andare ad analizzare automaticamente i difetti di ogni campione e cercare quindi eventuali correlazioni con le propietà fisiche del campione stesso. 
    Si è andati quindi a sviluppare una rete predittiva in grando di predire propietà chimico-fisiche a partire dalla conformazione strutturale e topologica dei campioni.
\end{abstract}

\chapter*{Introduzione}                 %crea l'introduzione (un capitolo
                                        %   non numerato)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries
% INTRODUZIONE}]{\fancyplain{}{\bfseries\thepage}}
% \lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
% INTRODUZIONE}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%aggiunge la voce Introduzione
                                        %   nell'indice
% \addcontentsline{toc}{chapter}{Introduzione}
L'Intelligenza Artificile e in particolare i modelli di Machine Learning basati sulle reti neurali nei tempi recenti si sono dimostrate tecnologie pervasive in quasi tutti gli ambiti di ricerca dimostrandosi estremamente flessibili e applicabili ai domini più eternogenei e disparati. \\

Non fa eccezione la ricerca sui materiali innovativi come quella sul grafene, un materiale composto da un singolo strato di atomi di carbonio posizionati in conformazione esagonale dando forma ai caratteristici reticoli.
Le propietà e caratteristiche eccezionali di questo materiale derivano da molte delle sue unicità, quali per esempio il suo sottilissimo spessore lungo solo un atomo e di conseguenza il suo peso estremamente contenuto. \\
Queste propietà \emph{bidimensionali} conferiscono al materiale resistenza, flessibilità e leggerezza senza concorrenza tra gli altri materiali tradizionali. \\
Le possibili applicazioni di un simile materiale sono anchesse estramamente multi-dominio e spaziano dai supercondensatori, semiconduttori, nonchè per usi strutturali e tanto altro. 
Per la natura stessa del materiale, la produzione su scala industriale risulta tutt'altro che banale e i costi elevati. \\
Spesso durante la produzione non si riesce ad ottenere un campione di grafene totalmente privo di impurità. E' inoltre comune trovare danni strutturali sul reticolo atomico con conseguente presenza di cavità sullo stesso. \\

L'idea del progetto quindi si è basata sull'analisi di campioni di grafene difettato derivanti da simulazioni. Grazie alla conoscenza a priori delle propietà fisiche di ogni campione, originariamente calcolate dalle simulazioni stesse è stato possibile mettere in correlazione la topologia e i tratti geomtrici dei suddetti difetti con le propietà del materiale stesso.\\
In primo luogo, prima di andare ad analizzare le geometrie dei vari difetti è risultato utile individuarli automaticamente. 
Questo è stato fatto addestrando la rete neurale YOLOv8, potente modello di Object Detection noto per la sua velocità di inferenza e per le elevate performace otenibili dopo un training relativaemnte breve e con campioni limitati.\\
Una volta individuate ed estratte dallo sfondo le regioni di interesse si è proceduto con l'analisi geomtrica e topologica dei difetti con la libreria di Computer Vision OpenCV.
Questa libreria multipiattaforma inizialmente sviluppata da Intel e divenuta successivamente open-source offre numerose funzioni e metodi per la manipolazione e analisi di immagini, nel caso di questo progetto ci si è interfacciati ad essa tramite Python. \\
E' stato quindi possibile estrapolare tratti geometrici e topologici dai difetti in questione calcolandone dai tratti più semplici come area e perimetro fino a quelli più sofisticati quali eccenticità, circolarità e molto altro.\\
I dati sono stati integrati in un dataframe \emph{Pandas}, grazie ad esso sono state fatte analisi sulle correlazioni fra i parametri stessi. Trovare parametri molto correlati fra loro è importante per evitare di selezionare valori ridondanti nell'addestramento della rete predittiva.\\

(Addestramento rete predittiva -- work in progress)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
\clearpage{\pagestyle{empty}\cleardoublepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
\rhead[\fancyplain{}{\bfseries\leftmark}]{\fancyplain{}{\bfseries\thepage}}
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
INDICE}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \listoffigures                          %crea l'elenco delle figure
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \listoftables                           %crea l'elenco delle tabelle
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}


%------------------------------------------------------------------------------------------------------------- CAPITOLO 1 -------------------------------------------------------------------------------------------------
\chapter{Introduzione al Machine Learning}                %crea il capitolo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
\lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries\rightmark}}
\pagenumbering{arabic}                  %mette i numeri arabi
La disciplina di ricerca sull'Intelligenza artificale pone le sue radici già a metà del XX secolo. L'obbiettivo generale di allora era quello di sviluppare macchine capaci di eseguire mansioni tipicamente attribute e svolte da umani. 
Con il passare dei decenni l'obbiettivo è rimasto lo stesso ma si sono esplorate numerose vie e tecniche differenti anche molto diverse tra loro, fra cui la sottocatgoria denominata come \emph{Machine Learning}. Quest'ultima è l'insieme delle tecniche e dei metodi statistici con i quali si va migliorare un algoritmo che ha come obbiettivo quello di identificare pattern nei dati. 
Questi metodi si sono riveati particolarmente utili in quei campi dove è notoriamente difficile se non impossibile sviluppare soluzioni tramite algoritmi tradizionali, per esempio il riconoscimento vocale e di immagini.\\
Negli ultimi decenni con il rapido avanzamento della velocità di calcolo a disposizione queste tecnologie hanno ricevuto una notevole accellerazione nello sviluppo e nei risultati raggiunti. 
In particolare ha preso sempre più popolarità l'approcio denomicanto come Deep Learning, ovvero quei metodi basati su reti neurali artificiali su più \emph{strati}. Ognuno di questi strati estrapola gerarchie di inforamzioni di un certo livello definite a loro volta su quelle di livello inferiore calcolate dagli strati precedenti.
\section{Reti neurali}
Una rete neurale consiste in un insieme di \emph{neuroni} (detti anche \emph{nodi}) collegati fra loro, che concorrono per risolvere un problema.
Ogni nodo può essere pensato come una semplice funzione di tante variabili di ingresso quanti sono i nodi collegati ad esso. \\
Il suo valore di attivazione quindi sarà la somma pesata dei valori di attivazione dei nodi che prende come input, più la somma di un valore chiamato \emph{bias}, il tutto all’interno di una \emph{funzione di attivazione} non lineare. \\
In formule l’attivazione di un neurone può essere espressa dalla semplice formula: \\
\begin{equation}
    y = \sigma(w \cdot x + b)
\end{equation}
Dove, \\
\begin{itemize}
    \item 
        $y$ è il valore scalare di attivazione del nodo, esso può variare con continuità da 0 a 1, dove 0 rappresenta la totale assenza di attivazione del nodo, mentre 1 rappresenta l'attivazione massima possibile. 
    \item 
        $ x= \begin{bmatrix} x_1 , x_2 , \cdot\cdot , x_n\end{bmatrix} $ rappresenta il vettore di input, ovvero i valori di attivazione $x_1, x_2, ... , x_n$ degli $n$ neuroni connessi al neurone in questione.
    \item 
        $ w= \begin{bmatrix} w_1 , w_2 , \cdot\cdot , w_n\end{bmatrix} $ della stessa dimensione di $x$
        reppresenta il vettore dei $pesi$, questi parametri sono importanti in quanto venendo moltiplicati per i rispettivi valori di input permettono di calcolare la media pesata dei valori di ingresso.
    \item 
        $b$ è detto $bias$, un valore che si va a sommare semplicemente alla somma pesata degli input.
    \item 
        $\sigma$ è la \emph{funzione di attivazione}, ne esistono di numerosi tipi, il suo compito principale è quello di aggiungere una componente di \emph{non-linearità} alla rete stessa. 

\end{itemize}
 Graficamente l'attivazione di un nuerone con soli tre input può essere rappresentata come: \\
 \begin{center}
     \begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2] 
  (x2) {$x_2$};
\node[on chain=2,join=by o-latex] 
  {$w_2$};
\node[on chain=2,init] (sigma) 
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Funzione di \\ attivazione}}]   
  {$\sigma$};
\node[on chain=2,label=above:Output,join=by -latex] 
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm) 
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex] 
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm) 
  (x3) {$x_3$};
\node[on chain=3,label=below:Pesi,join=by o-latex] 
  (w3) {$w_3$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
\end{tikzpicture}
 \end{center}

Sempre in formule si può rappresentare l'attivazione del neurone in forma estesa come: 
\begin{equation}
    y = \sigma(w_1  x_1 + w_1  x_1 + .. w_n  x_n + b) = \sigma(\sum_{n=1}^{n} w_n x_n + b)
\end{equation}
La funzione di attivazione è necessaria innanzitutto per trasformare un valore \emph{unbunded}, ovvero senza un valore massimo, derivante dalla sommatoria degli input in un valore continuo che va da 0 a 1, ovvero il valore di attivazione finale che vogliamo fare assumere al neurone del caso. \\
Esistono numerose funzioni di attivazione, una delle più famose e semplici è la \emph{Sigmoide} \[ S(x) = \frac{1}{1+e^{-x}}\] simile alla funzione \emph{gradino unitario} ma in versione smussata, senza la discontinuità nello zero. Questa funzione rispetto al \emph{gradino unitario} ha vari vantaggi, in particolare a piccole variazioni dell'input avremo piccole variazioni nell'output. \\
Nelle reti più moderne è sempre più utilizzata la RELU, ovvero la funzione rettificatore espressa semplicemente come: 
\begin{equation}
     f(x)=\max(0,x)
\end{equation}
Quest'ultima si è dimostrata come la più efficace nelle reti di apprendimento profondo. Fra i suoi vantaggi abbiamo l'attivaazione scarsa, infatti essa si attiva solo nella metà dei campioni di una rete inizializzata casualmente inoltre si hanno meno problemi in fase di discesa del gradiente nella fase di adestramento rispetto alla funzione Sigmoide dei quali parleremo più avanti nel capitolo.
\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{gradino.png}

(a) Gradino unitario
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{sigmoid.png}

(b)  Sigmoide
\end{minipage}
\caption{Comparazione fra funzione gradino unitario e sigmoide }
\end{figure}

Esistono poi funzioni di attivazione più sofisticate, come la Softmax.
Quest'ultima è spesso utile nelle reti classificatrici multiclasse. In questi casi i nodi del layer di output rappesentano le probabilità di classificazione per ciascuna classe. In questi casi è cruciale che la sommatoria dei valori di attivazione dei nodi di output sia uguale a 1.
\newpage
Nel caso generale abbiamo almeno un layer di nodi di input, uno di output e acluni layer intermedi detti \emph{hidden-layer}.
In una rete \emph{fully-connected} tutti i nodi nell n-esimo layer sono connessi a tutti i nodi dell'n-esimo layer. \\
Le feature dei vari pattern estrapolati dalla rete vengono calcolati dai nodi dei layer interni i quali estrapolano propietà sempre più astratte e di alto livello quanto più è avanzata la posizione del layer in cui risiedono. \\
Il vettore dei valori di attivazione di un \emph{hidden-layer} può essere espresso come il prodotto tra la matrice dei pesi del layer in questione e il vettore di attivazione del layer precedente più il vettore dei bias, tutto dentro la funzione di attivazione scelta per questo layer.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{neural_networks-004.png}
\caption{Rete neurale \emph{fully-connected} con $3$ \emph{hidden layers} da $m$ nodi ciascuno.}
\end{figure}

\newpage
In formule il vettore dei valori di attivazione del primo \emph{hidden-layer} può essere espresso come:
    \begin{equation} 
     \begin{pmatrix}
        a_{1}^{(1)} \\[0.3em]
        a_{2}^{(1)} \\
        \vdots \\
        a_{m}^{(1)}
      \end{pmatrix}
    =
      \sigma \left[ 
      \begin{pmatrix}
        w_{1,0} & w_{1,1} & \ldots & w_{1,n} \\
        w_{2,0} & w_{2,1} & \ldots & w_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        w_{m,0} & w_{m,1} & \ldots & w_{m,n}
      \end{pmatrix}
      {
      \begin{pmatrix}
        a_{1}^{(0)} \\[0.3em]
        a_{2}^{(0)} \\
        \vdots \\
        a_{n}^{(0)}
      \end{pmatrix}}
      +
      \begin{pmatrix}
        b_{1}^{(0)} \\[0.3em]
        b_{2}^{(0)} \\
        \vdots \\
        b_{m}^{(0)}
      \end{pmatrix}\right] 
      \\
    \end{equation}
In notazione compatta abbiamo: 
    \begin{equation}
        \mathbf{a}^{(1)} = \mathbf{W}^{(0)} {a^{(0)}}+\mathbf{b}^{(0)}
    \end{equation}

In generale non tutti le reti sono \emph{fully-connected}, spesso si opta per una soluzione ibrida dove si utilizzano strati con architetture diverse per risolvere un problema. 
Un esempio di un'altra archiettura di rete sono le reti ricorsive (RNN), queste hanno dei collegamenti in retroazione capaci di mantenere uno \emph{stato} e quindi avere \emph{memoria}. Mantenere una memoria è cruciale quando gli imput passati in precedenza nella rete in relazione con quelli attuali hanno un significato semantico importante, come per esempio nell'analisi di un video o per un riconoscimento vocale partendo da un file audio.
Un'altro tipo di rete molto importante e centrale in questo lavoro di tesi sono le reti convoluzionali CNN. Esse si basano su filtri convoluzionali che collegano gli \emph{hidden-layer}.\\ Queste reti riescono a estrapolare pattern grafici a partire da un'immagine fornita come imput, per esempio utilizzando il valore numerico associato ad ogni pixel in un'immagine in scala di grigi.\\
Questo tipo di reti sono state usate nell'ambito di questo progetto per identificare automaticamente le aree di interesse dove sono presenti i difetti del grafene. Di queste reti e in particolare di YOLOv8 ne parleremo nel dettaglio nel prossimo capitolo.

\newpage
\section{Algoritmi di apprendimento}
\subsection{Funzione di costo}
Uno dei parametri più importanti nonchè alla base degli algoritmi di apprendimento è la cosidetta \emph{funzione di costo}.
Ne esistono di vari tipi ma intuitivamente essa è una funzione reale che prende come variabili i parametri del sistema $C(\omega,b)$ e fornisce un'indicazione su quanto bene la nostra rete stia performando sui campioni che gli vengono dati.\\
Questa valutazione è possibile confrontando i valori $y(x)$ che la rete produce per gli imput $x$, conoscendo il vettore $a(x)$ dei risultati attesi per gli imput in questione.
Una delle più utilizzate è lo \emph{scarto quadratico medio}, essa fa la media aritmetica del quadrato dell'errore di output rispetto a ogni imput.
\begin{equation}
    C(\omega,b) = \frac{1}{n} \sum_{n=1}^{n} (y(x_{i}) - a_{i}))^2
\end{equation}
Al variare dei numerosi parametri $(W,b)$ la performance della rete varia e così anche la funzione di costo. In particolare più il valore si avvicina a zero e più la rete sta performando bene. \\
Risulata quindi chiaro che minimizzare la funzione di costo $C(\omega,b)$ è il fulcro del problema dell'addestramento delle reti neurali.
\subsection{Discesa del gradiente}
Siamo quindi di fronte a un problema di ottimizzazione di una funzione reale in tante variabili quanti sono i parametri della rete, dove si cerca quindi un minimo della funzione stessa.
Intuitivamente l'inverso del gradiente ci da un'indicazione \emph{spaziale} su dove dobbiamo \emph{muoverci} per avvicinarci il più velocemente possibile al più vicino minimo locale, quindi per quale vettore di valori dobbiamo sommare i nostri parametri $(\omega,b)$ per migliorarli. \\
\newpage
Le iterazioni del metodo si presentano nella forma seguente: prima si va a calcolare l'inverso del gradiente, lo si moltiplica per una certa costante $\eta$, quindi lo si somma ai parametri $(\omega,b)$. In questo modo si vanno ad ottenere i nuovi parametri $(\omega', b')$. \\
L'equzione seguente mostra un'iterazione del processo di discesa del gradiente, dove vengono calcolati i nuovi parametri $(\omega', b')$:
\begin{equation}
    (\omega', b') := (w,b)-\eta \nabla C(w,b)
\end{equation}
Importante notare come l'entità dello \emph{spostamento} per ogni step è arbitrariamente piccola rispetto al valore di $\eta$. Quest'ultimo prende il nome di \emph{learning-rate} ed è un \emph{iperparametro} in quanto è una variabile scelta a priori da colui che sta addestrando la rete. \\
Un'altro accorgimento per evitare fluttuazioni imprevedibili dei parametri è quello di non applicare i cambiamenti ad ogni predizione ma fare la media su un batch di più predizioni. \\
Il metodo illustrato sopra prende il nome di \emph{Gradient Descent GD}\cite{K1}, questo è un metodo applicabile quando la funzione di costo è sufficientemente semplice e il suo gradiente è di facile computazione.\\
Nel \emph{GD} si calcola il gradiente per l'intero set di parametri ed essi vengono aggiornati ad ogni iterazione oppure facendo la media su batch di un certo numero di predizioni. \\
Quando invece ho dataset molto grandi oppure non ho un'espressione della funzione di costo abbastanza semplice si può optare per il metodo noto come \emph{Stochastic Gradient Descent SGD}.
Questo metodo approssima il calcolo del gradiente considerando ogni volta un batch casuale di paramertri ad ogni iterazione, solo una parte dei parametri quindi vengono aggiornati durante un'iterazione.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{Stogra.png}
\caption{Esempio di andamento della funzione costo nel corso delle iterazioni in un problema di ottimizzazione risolto tramite SGD}
\end{figure}
\newpage
Come si può vedere dall'andamento della funzione di costo durante le iterazioni del SGD, non è assicurato il miglioramento della rete ad ogni iterazione, in alcuni casi infatti la funzione di costo può momentaneamente salire, questo è dovuto alla natura stocastica, quindi probabilistica del metodo.\\
In generale comunque, per computare la derivata parziale di un parametro occorre un passaggio e il tutto deve essere ripetuto per ogni epoca. \\
Avendo spesso miliardi di parametri e centinaia di epoche i metodi di GD possono risultare facilemente costosi computazionalemente. \\
Per mitigare questo è stato creato un nuovo algoritmo utilizzabile insieme al SGD, esso prende il nome di \emph{Backpropagation}\cite{backprop}.
Questo algoritmo ricorsivo sfruttando la regola della \emph{chain rule} (durante il calcolo del gradiente) permette di calcolare tutte le derivate e quindi l'intero gradiente in un singolo passaggio.\\

Qui sotto mostro la differenza intuitiva fra la progressione di un algoritmo di GD a batch con uno di SGD.\\
Notare bene come la visualizzazione è possibile solo se si considera il caso estremamente semplificato di soli due parametri. \\Nel caso generico con più di due parametri (anche miliardi) sarebbe impossibile proporre una visualizzazione di uno spazio con così tante dimensioni.
\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{batchGD.png}

(a) Batch Gradient Descent
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{SGD.png}

(b)  Stochastic Gradient Descent
\end{minipage}
\caption{Comparazione fra iterazioni di GD a batch e SGD }
\end{figure}
Notare la natura intrisecamente probabilistica degli step del SGD, essi non assicurano un miglioramento della loss ad ogni step ma il risulato finale può risultare comunque più efficente della controparte GD se analizzato nel complesso.
\newpage

\section{Esempi applicativi di Reti Neurali}
Le reti neurali hanno numerose ricadute per la risoluzione di problemi applicativi di ogni genere.
Forse il più famoso caso applicativo in letteratura e sicuramente il più usato per introdurre le Reti Neurali Convoluzionali (\emph{CNN}) è il problema del riconoscimento automatico di numeri (cifre) scritte a mano. 
Le CNN verranno approfondite nel prossimo capitolo, in particolare verranno inquadrate nella classe di problemi di Object Detection, ma vale comunque la pena introdurre fin da subito la risoluzione di questo ben noto caso di studio. 
\subsection{Classificazione di cifre scritte a mano}

Il dataset di riferimento è il ben noto \emph{MNIST}.\\
Le immagini contenute rappresentano semplici numeri a una cifra scritti a mano, ognuno di questi viene fornito con la relativa annotazone (\emph{label}) scritta da operatori umani.\\
Questo database contiene 60.000 immagini 28x28 pixel in scala di grigi per la fase di training e ulteriori 10.000 per la fase di valutazione del modello. \\
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{minst.png}
\caption{Database MINST}
\end{figure}
La valutazione di ogni modello di reti neurali non deve essere effettuata sugli stessi dati di addestramento, ma su altri che il modello non ha mai visto prima. Ciò che si vuole valutare infatti è la capacità acquisita dal sistema nel riconoscimento di pattern generali estrapolabili da campioni generici. Se utilizzassimo gli stessi dati usati nell'adestramento per valutare la rete, rischieremmo di incorrere nella possibilità che la rete sia performate nell'analisi dei dati di training ma incapace di generalizzare su dati generici mai visti prima.   \\
\newpage
L'imput layer è composto da tanti nodi quanti sono i pixel dell'immagine, ognuno di questi prende il valore numerico in scala di grigi del pixel in questione.
L'imput layer quindi, si deve pensare come la matrice di pixel dell'immagine iniziale. Nel caso avessimo un'immagine a colori, per esempio in formato RGB, al posto di un imput layer sotto forme di matrice, ne avremo 3, una per ogni colore (rosso, verde, blu), lavoreremo quindi con tensori.\\
\newpage
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{pixel-values.png}
\caption{Imput layer di una CNN per analisi immagine}
\end{figure}
Questo problema, come in tutti i casi in cui si cercano oggetti e/o pattern all'interno di immagini, si risolve impiegando strati convoluzionali. \\
Questi strati di neuroni sono composti da \emph{filtri} che \emph{scorrendo} sui valori di attivazione dei layer prededenti computano operazioni di convoluzione estrapolando tratti \emph{grafici} dalle immagini.\\
La natura di questi \emph{filtri} e delle operazioni di convoluzione effettuate da essi verranno approfonditi e esplorati nel prossimo capitolo. 
In generale ci aspettiamo che layer convoluzionali di strati più avanzati estrapolino tratti grafici più astratti e complessi sulla base dei tratti di più basso livello estratti dai livelli inferiori. Nel nostro caso è plausibile che i primi livelli riconoscano feature di basso livello come i bordi delle cifre, mentre quelli successivi riconoscano pattern più strutturati come per esempio i \emph{cerchi} dello zero e dell'otto. 
\newpage

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{digit_arch.png}
\caption{Modello di CNN per riconoscimento di cifre scritte a mano}
\end{figure}
Come mostrato in figura, spesso fra un layer convoluzionale e il successivo sono presenti dei layer di altra natura, come quelli di \emph{pooling}. Ne esistono di vari tipi ma anchessi si comportano come dei filtri che scorrono sulle immagini prodotte dai filtri convoluzionali precedenti. Il loro scopo è quello di diminuire le dimensione delle immagini prodotte mantenedo parte dell'informazone grafica, magari enfatizzando delle strutture geometriche piuttosto che altre.

Gli ultimi layer invece tornano ad essere \emph{fully-connected} e vengono chiamati \emph{DL, Dense Layers.}
In particolare l'ultimo layer si occupa della classificazione della cifra data come input. Come spiegato in precedenza in questi casi si utilizzano funzioni di attivazione con un comportamento similare alla Softmax. La sommatoria dell'attivaizone dei 10 nodi dell'output layer deve essere infatti pari a 1, dato che ciascunio dei valori di attivazione rappresenta la probabilità che l'immagine di imput corrisponda alla cifra simboleggaita dal suddetto nodo.\\
\newpage
In conclusione possiamo dire che l'applicazione di una CNN a questo tipo di problema si rivela la scelta migliore, in generale risulta la scelta più performante in tutti i tipi di probelmi dove l'imput è composto da un'immagine.\\
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{cnn_perf.png}
\caption{Modello di CNN per riconoscimento di cifre scritte a mano}
\end{figure}
In questo caso di studio già dopo una poche decine di epoche la precisione della classificazione si avvicina al 100\%.
Proseguire oltre non è necessario e anzi può risultare controproducente in quanto si potrebbe andare incontro a problemi di overfitting.
Con overfitting si intende ls situazione in cui il modello si adatta troppo ai dati di training perdendo generalità e capacità di astrarre su dataset mai visti prima. \\

%------------------------------------------------------------------------------------------------------------- CAPITOLO 2 -------------------------------------------------------------------------------------------------
{\let\clearpage\relax \chapter{Object Detection, YOLO e applicazioni su campioni di nano grafene}}

In questo capitolo andremo a introdurre la classe di problemi di visione artificiale denominata come Object Detection. Ovvero le soluzioni e i metodi che puntano a individuare, classificare o comunque riconoscere oggetti e/o pattern all'interno di un'immagine o un video. \\
Questa clase di problemi, come accennato nel capitolo precedete si è rivelata molto adatta all'impiego delle CNN (Convolutional Neural Networks), che grazie alla loro capacità di riconiscemnto ed estrapolazione di fetaures grafiche dalle immagini riescono a ottenere ottime performance su questa classe di problemi.\\
Dopo un'introduzione all'architttura delle CNN, ci focalizzeremo sullo studio di \emph{YOLO (You Only Look Once)}, una CNN allo stato dell'arte per quanto riguarda l'Object Detection, particolarmente nota per il basso tempo di inferenza e per questo particolarmente utile nelle applicazioni in tempo reale.\\
Analizzeremo inoltre l'applicazione di questo modello al caso di studio del progetto su cui si basa questa tesi, ovvero sul riconoscimento automatico di difetti su campioni di nano grafene, riportando metodologia e risultati.
\section{Reti neurali convoluzionali}
La struttura delle CNN è composta da vari tipi di layer di natura diversa, come i layer convoluzionali, i pooling layer e i fully-connected layers. 
Ogni tipologia di layer svolge ruoli e opera in modo diverso rispetto agli altri al fine di raggiungere l'obbiettivo preposto.
\\L'architettura generale può essere espressa come come nella figura sottostante.\\

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{CNN_main.jpeg}
\caption{Archiettura di una CNN a singolo canale}
\end{figure}
In questa trattazione delle CNN ci limiteramo ad analizzare il caso in cui l'immagine di input sia in scala di grigi e quindi il layer di input sarà composto da matrice di interi con valori compresi tra 0 e 255.
Nonostante questa semplificazione è importante sapere che il tutto può essere esteso ad immagini a più canali introducendo ulteriori matrici, una per ogni canale aggiuntivo. Generalmente quindi si avranno tensori con 3 dimensioni di profondità. 
\newpage
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{greyscaletorgb.png}
\caption{Passaggio da matrice di imput in scala di grigi a tensore per immagine RGB}
\end{figure}

In questa sezione andremo ad analizzare un tipo di layer alla volta, iniziando dai layer convoluzionali.
\subsection{Layer convoluzionali}
I componenti fondamentali dei layer convolzuonali non sono più vettori di neuroni ma fetaure map (matrici o tensori) computate da dei \emph{filtri} tramite operazioni di convoluzione discreta.
Questi filtri si presentano come delle matrici di dimensione minore della matrice di input, il loro comportamento è quello di \emph{scorrere} su tutta l'immagine di partenza andando a creare un'altra matrice, di dimensione minore chiamata \emph{feature map.}
L'operazione di convololuzione si può esprimere com la sommatoria del prodotto puntuale di ogni valore del filtro con i rispettivi valori nella matrice di partenza, secondo la posizione del filtro.
\newpage
In formule posiamo indicare l'attivazione di ogni nodo (pixel) della feature map come: 
\begin{equation}
    o_{i,j} = \sigma(b + \sum _{l=0}^{n}\sum _{m=0}^{n}\omega_{l,m}a_{i+l, j+m})
\end{equation}
dove $o_{i,j}$ è l'attivazione dell'elemento della matrice di output, $\sigma$ è una qualsiasi funzione di attivazione, $b$ l'eventuale bias, $\omega_{l,m}$ il valore dell'elemento del filtro nella posizione $l,m$ e $a_{i+l, j+m}$ il valore del pixel dell'immagine di input che verrà moltiplicato per $\omega_{l,m}$ durante lo step.

\begin{figure}[h]
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{filtera.png}

(a) Prima convoluzione  
\end{minipage}%
\hfill\vrule\hfill
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{filterb.png}

(b) Seconda convoluzione
\end{minipage}
\caption{Prime due iterazioni di convoluzione per la creazione di una feature map, con un filtro 3x3, no padding e stride 1}
\end{figure}
La feature map prodotta risulta più piccola dell'immagine di partenza, questo può essere un problema se si applicano numerose convoluzioni in sequenza andano quindi a ridurre sempre di più la dimensione delle immagini prodotte. Per evitare questo si può applicare la tecnica chiamata \emph{padding}, ovvero l'aggiunta a  posteriori di uno strato esterno alla matrice dell'immagine con valori nulli (\emph{zero padding}. Allo scorrere del filtro quindi si otterrà un'immagine della stessa dimensione di partenza.\\
Lo \emph{stride} invece è la dimensione dello \emph{step} del filtro durante lo scorrimento, solitamente è pari a 1, se lo si aumentasse si otterebbero anche in questo caso feature map più piccole dell'originale.

Questi filtri nei primi layer sono in grado di estrarre tratti di basso livello come i bordi e contorni degli oggetti. In generale sono molto adatti per enfatizzazione contorni di oggetti sull'immagine di partenza in modo da rendere più facile le operazioni dei layer successivi.
Nell'immagine successiva si mostrano varie feature map derivate da diversi filtri diversi. E' chiaro come l'operazione di convoluzione applicata da specifici filtri sia in grado di mettere in risalto contorni diversi rispetto ad altri.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fetaureMaps.png}
\caption{Fetaure map generate da diversi filtri a sulla stessa immagine}
\end{figure}
Il risulatato dell'operazione lineare di convoluzione viene quindi passato per una funzione di attivazione non lineare come la ReLU, discussa nel capitolo precedente.\\

I parametri fondamentali di apprendimento delle CNN sono propio i pesi (quindi gli elementi di matrice) dei filtri convoluzionali, la rete quindi apprenderà i valori ottimali per ciascun filtro allo scopo di estrapolare i pattern richeisti dal problema.
\\
Invece, la dimensione e il numero dei filtri, il padding e lo stride sono \emph{iperparametri}, ovvero sono valori decisi a priori da colui che sta addestrando la rete, prima del processo di training stesso. 
\newpage
\subsection{Layer di pooling}
Dopo una serie di convoluzioni, tipicamente, si va ad applicare un layer di \emph{pooling}, ovvero un'altra operazione lineare simile alla convoluzione dalla quale si ottengono immagini più piccole ma che comunque mantengono un certo contenuto informativo posizionale della feature map iniziale.
Esistono varie tipologie di pooling con comportamenti diversi tra loro, nell'immagine sottostante si mostra il funzioanamento della tipologia di gran lunga più utilizzata, il \emph{Max-pooling}.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{maxpooling.png}
\caption{Maxpooling 2x2, stride 2}
\end{figure}
Il funzionamento del max-pooling è similare a quello di un filtro convoluzionale che scorre su una feature map, ma al posto di fare la convoluzione dei valori, prende il valore più grande fra quelli coperti dalle dimensioni del filtro e lo riporta nella nuova immagine.\\

Il \emph{pooling}, più in generale, è quindi un processo di scalamento della feature map di partenza introducendo una invarianza a piccole distorsioni e variazioni dell'immagine stessa mantenendo però gran parte delle informazioni spaziali della dell'immagine iniziale.\\
Grazie a questo processo è possibile ridurre notevolemente i parametri di apprendimento della rete. Notare inoltre che i parametri di pooling come dimensione dei filtri, stride e padding sono tutti iperparametri che non vengono appresi dalla rete in fase di training.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{maxPoolingResult.png}
\caption{Risultato di un'operazione di max-pooling 2x2 con stride 2}
\end{figure}
\\Notare come se si va ad applicare un layer di max-pooling 2x2 con stride 2, la feature map di partenza viene dimezzata come dimensioni ma il contenuto semantico dell'immagine rimane intatto.

\subsection{Fully-connected layer }
Gli ultimi layer della rete sono fully-connected, del tutto simili a quelli introdotti nel capitolo 1. \\
Le feature map prodotte dall'ultimo layer convoluzionale/pooling vengono \emph{appiattite}, ovvero inserite in un semplice array unidimensionale (un vettore) e connessi a uno o più layer fully-connected. In questo layer per ogni connessione fra un neurone e l'altro c'è un peso, che la rete andrà ad apprendere durante l'addestramento, inoltre l'attivazione del singolo neurone viene passata tipicamente attraverso la ReLU. Le feature grafiche estratte dai layer precedenti sono finalmente mappate tramite i layer fully-connected al layer di output che rappresenta le probabilità di classificazione dell'immagine.
\\Il layer di output viene anchesso passato per una funzione di attivazione, tipicamente diversa da quelle utilizzate dei dense-layer precedenti. Nel caso della classificazione multipla si usa la Softmax, nel caso della classificazione binaria la sigmoide.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{CNN_fully_connected.png}
\caption{CNN con fully connected layer nella parte finale per classificazione multiclasse di oggetti }
\end{figure}

Nei problemi di classifcazione multiclasse il layer di output può essere trattato come una distribuzione di probabilità. Ovvero la rete non si limita a fornire un giudizio sulla classe presente nell'immagine, ma fornisce una probabilità per ogni classe presente nel problema.
Per valutare la performace della rete è utile quindi confrontare la distribuzione di probabilità prodotta dalla rete con quella attesa.
\newpage
Viene usata quindi una funzione di costo diversa da quella trattata nel primo capitolo, chimata funzione di cross-entropia.
\begin{equation}
    C\left(p\|q\right)=-\sum _{i}^{M}p_{i}\log _{2}q_{i}
\end{equation}
All'interno del contesto di una predizione per un'immagine in input, $M$ rappresenta il numero di classi globali, $p_{i}$ rappresenta il valore della classe $i$ nella distribuzione dei valori attesi, e $q_{i}$ rappresenta il valore della classe $i$ nella distribuzione prodotta dalla rete (predizione).\\
La formuala nel caso pratico dei problemi di clasificazione si semplifica in quanto i valori di $p_{i}$ sono sempre nulli per tutte le classi meno che per la classe effettivamente indicata nella fase di labeling per quell'immagine dove invece è pari a 1. \\ Quindi possiamo riscrivere la formula come: 
\begin{equation}
    C\left(p\|q\right)=-\sum _{i}^{M}\log _{2}q_{i}
\end{equation}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{cross_entropy.png}
\caption{Comportamento della funzione di costo in base alle predizioni del modello}
\end{figure}
La cross-entropy si è rivelata molto adatta nei problemi di classificazione multiclasse per la sua rapida pendenza nei casi in cui la rete assegna una scarsa probabilità alla classe presente nel'immagine. In questi scenari infatti le magnitudine delle sue derivate inducono a rapide variazioni dei parametri di apprendimento. In questo modo gli algoritmi di ottimizzazione riescono a migliorare le performace della rete con rapidità.
\newpage
Concludo la sezione osservando che nelle CNN è fondamentale scegliere a priori un buon set di iperparametri e di architettura dei layer adatto per risolvere il problema in questione, compresa la funzione di costo e l'ottimizzatore. \\

Nella prossima sezione andremo ad analizzare il problema dell'Object Detection e l'architettura di una nota rete di Object Detection, YOLO.
\newpage
\section{Object Detection e YOLO}

% \clearpage{\pagestyle{empty}\cleardoublepage}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%per fare le conclusioni
% \chapter*{Conclusioni}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries
% CONCLUSIONI}]{\fancyplain{}{\bfseries\thepage}}
% \lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries
% CONCLUSIONI}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%aggiunge la voce Conclusioni
%                                         %   nell'indice
% \addcontentsline{toc}{chapter}{Conclusioni} Queste sono le
% conclusioni.\\
% In queste conclusioni voglio fare un riferimento alla
% bibliografia: questo \`e il mio riferimento \cite{K3,K4}.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \renewcommand{\chaptermark}[1]{\markright{\thechapter \ #1}{}}
% \lhead[\fancyplain{}{\bfseries\thepage}]{\fancyplain{}{\bfseries\rightmark}}
% \appendix                               %imposta le appendici
% \chapter{Prima Appendice}               %crea l'appendice
% In questa Appendice non si \`e utilizzato il comando:\\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\verb"" è equivalente all'
%                                         %   ambiente verbatim,
%                                         %   ma si utilizza all'interno
%                                         %   di un discorso.
% \verb"\clearpage{\pagestyle{empty}\cleardoublepage}", ed infatti
% l'ultima pagina 8 ha l'intestazione con il numero di pagina in
% alto.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries \thechapter \:Prima Appendice}]
% {\fancyplain{}{\bfseries\thepage}}
% \chapter{Seconda Appendice}             %crea l'appendice
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%imposta l'intestazione di pagina
% \rhead[\fancyplain{}{\bfseries \thechapter \:Seconda Appendice}]
% {\fancyplain{}{\bfseries\thepage}}
\begin{thebibliography}{90}             %crea l'ambiente bibliografia
\rhead[\fancyplain{}{\bfseries \leftmark}]{\fancyplain{}{\bfseries
\thepage}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%aggiunge la voce Bibliografia
                                        %   nell'indice
\addcontentsline{toc}{chapter}{Bibliografia}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%provare anche questo comando:
%%%%%%%%%%%\addcontentsline{toc}{chapter}{\numberline{}{Bibliografia}}
%GD e SGD
% textbook generale, molto completo sulle NN 
\bibitem{NN_book} R. Rojas, \emph{Neural Networks}, Springer-Verlag, Berlin, 1996
\bibitem{GD} Bottou, Léon; Bousquet, Olivier (2007). \emph{The Tradeoffs of Large Scale Learning}
%articolo sulla backpropagation
\bibitem{backprop} Annette Lopez Davila, Professor Chi-Kwong Li, \emph{Neural Networks: The Professor Chi-Kwong Li Algorithm} Math 400, College of William and Mary
%articolo sull'esempio del capitolo 1
\bibitem{handwritten} Ritik Dixit, Rishika Kushwah, Samay Pashine, \emph{Handwritten Digit Recognition using Machine and Deep Learning Algorithms}, Computer Science and Engineering
Acropolis Institute of Technology \& Research
Indore, India

%-----------------------secondo capitolo ------------------------

\bibitem{cnn_overview}Rikiya Yamashita, Mizuho Nishio, Richard Kinh Gian Do Kaori Togashi \emph{Convolutional neural networks: an overview
and application in radiology}, Insights into Imaging volume 9, pages 611–629 (2018)

\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%non numera l'ultima pagina sinistra
% \clearpage{\pagestyle{empty}\cleardoublepage}
% \chapter*{Ringraziamenti}
% \thispagestyle{empty}
% Qui possiamo ringraziare il mondo intero!!!!!!!!!!\\
% Ovviamente solo se uno vuole, non \`e obbligatorio.
\end{document}
